\documentclass[a4paper, 10pt, american, titlepage]{article}

% useful packages
\usepackage[utf8]{inputenc} % UTF-8 support
\usepackage{minted} % for code snippets
\usepackage[american]{babel} % for changing particular titles
\usepackage{csquotes} % recommended for biblatex
\usepackage{graphicx} % for images
\usepackage[gen]{eurosym} % for literally just the euro symbol
\usepackage{lipsum} % lorem-ipsum placeholder text
\usepackage{bookmark} % links to other parts of the PDF
% if we want to use a different style, here are some to look at
% https://www.overleaf.com/learn/latex/Biblatex_bibliography_styles
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex} % citations
\usepackage[page, titletoc, title]{appendix} % labeled appendices
\usepackage[margin=1in]{geometry} % set 1in margins
\usepackage{hyperref} % hyperlinks
\usepackage{setspace} % setting custom spacing
\usepackage{tabulary} % nicer tables
\usepackage{CJKutf8} % for Japanese UTF-8 characters

% more breathing room
\setminted{fontsize=\small,baselinestretch=1}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% stuff for LaTeX to know
\bibliography{references}
\graphicspath{ {./images/} } % put images in here
\title{%
	\huge ARuko and Editour: \\
	\Large A Platform for Augmented Reality Tour Guide Apps}
\author{William~Campbell, Cole~Granof, and Joseph~Petitti}
\date{October 11, 2019}

% uncomment this to call it "Table of Contents" instead of just "Contents"
% \addto{\captionsamerican}{\renewcommand*{\contentsname}{Table of Contents}}

% custom environment for indenting whole paragraphs
\newenvironment{indented}[1]%
{\begin{list}{}%
	{\setlength{\leftmargin}{#1}}%
	\item[]%
}
{\end{list}}

\begin{document}

% set page numbers to Roman for the forematter (before the introduction)
\pagenumbering{roman}

% title page
\begin{center}
	{\huge ARuko and Editour: \\
		\Large A Platform for Augmented Reality Tour Guide Apps
	}

	\vfill

	A Major Qualifying Project \\
	Submitted to the faculty of \\
	WORCESTER POLYTECHNIC INSTITUTE \\
	In partial fulfillment of the requirements for the \\
	Degree of Bachelor of Science \par

	\vfill

	By \\
	William Campbell \\
	Cole Granof \\
	Joseph Petitti \par

	\vfill

	Date: \\
	October 11, 2019 \par

	\vfill

	Report submitted to: \par

	\vspace{\baselineskip}

\end{center}
\begin{flushright} % list your sponsors and advisers
	Atticus Sims, Sponsor \\
	Kyoto VR \par
	\vspace{\baselineskip}
	Professor Joshua Cuneo, Adviser \\
	Worcester Polytechnic Institute \par
	\vspace{\baselineskip}
	Professor Ralph Sutter, Adviser \\
	Worcester Polytechnic Institute \par
\end{flushright}

\vfill

\clearpage

% more breathing room
\setstretch{1.5}

\begin{abstract}
Kinkaku-ji, the Temple of the Golden Pavilion, is one of Kyoto's most visited
tourist attractions. Most people who visit the site do not have the opportunity
to experience the rich art and culture that are part of its history. We worked
with our sponsor, Kyoto VR, to create an app that would allow visitors of 
Kinkaku-ji to better experience these aspects of the site. We used Unity to
create an audio tour app that provides historical insight on the location, 
displays galleries of art throughout the visit, and offers accessibility 
features through AR. We hope our app will enhance the experience of visiting
Kinkaku-ji.  
\end{abstract}

\section*{Acknowledgments}
\label{sec:acknowledgements}
\addcontentsline{toc}{section}{Acknowledgments}

Our team would like to thank Atticus Sims, CEO and founder of Kyoto~VR, for
sponsoring this project and providing help and guidance throughout our time in
Japan. His knowledge of the history of Kinkaku-ji and Kyoto in general has been
invaluable to our experience.

We would also like to thank Inoue Hikaru for his help with translations and
researching AR technology. A special thank you to the IQP team working with
Kyoto~VR, Lewis Cook, Nicole Escobar, Ahad Fareed, and Cameron Person, for
filming and editing the app trailer, as well as providing help with testing.
Further, we are grateful to the assistance and resources provided by Ritsumeikan
University and Professor Noma Haruo. Finally, a thank you to our advisers,
Professors Joshua Cuneo and Ralph Sutter. Without their expert advice and
feedback this project probably could not have been completed.

\clearpage

\section*{Executive Summary}
\label{sec:executiveSummary}
\addcontentsline{toc}{section}{Executive Summary}

Our team consists of three Senior Worcester Polytechnic Institute (WPI)
students: William Campbell, computer science and interactive media and game
development double major; Cole Granof, computer science major; and Joseph
Petitti, computer science major. We spent twelve weeks in Shiga prefecture, Japan,
completing the requirements for our Major Qualifying Project, a WPI graduation
requirement in which students solve a real-world problem through practical
project-based work. Our project was sponsored by Kyoto~VR, a startup founded in
2016 to ``document and preserve the culture and heritage of the city and
communicate the richness of Japan’s Old Capital to the
world''~\autocite{kyotovr2018}.

Our team worked directly under Kyoto VR founder and CEO Atticus Sims to develop
and test a mobile app to deliver an audio tour, incorporating various
augmented reality (AR) features to enhance the experience. Our app delivers a
tour of Kinkaku-ji, or The Temple of the Golden Pavilion, a popular tourist
destination in Kyoto. In this app, which we call ARuko, many of the AR features
were aimed at improving accessibility. These features include translating signs
and providing additional info on the physical map located at the site. Other
features were purely for spectacle, such as the ability to take an AR
``souvenir'' home by scanning the ticket to enter Kinkaku-ji.

ARuko went through multiple iterations of user testing and bug testing at
Kinkaku-ji. Sections~\ref{sec:initialFieldTesting} and
\ref{sec:finalFieldTesting} go into detail about our two days of field testing
on-site. From these tests, we were able to refine the user experience, and
identify important features to add to the app. Much of the app's functionality
was decided by our project sponsor, who plans to monetize the app. We also
worked with an IQP team, who had been tasked with exploring funding options and
creating promotional content.

Since our app had to deliver an audio tour supplemented by images based on your
latitude and longitude, we developed an internal tool to design physical
regions for each step of the tour. This tool, called Editour, provides a rich
editing suite through a simple web interface. Section~\ref{sec:editingATour}
goes into detail about the features of this web app. We also wrote a backend to
manage the tour files, allowing one of us (or our sponsor, Atticus Sims) to
download a zip file containing all of the media and info to reconstruct the
tour. This zip can be extracted into the source files for ARuko, which rebuilds
the tour from the downloaded contents. We were able to host the backend on the
server provided by Kyoto~VR's hosting plan; we discuss the hurdles that we
overcame to deploy the Editour backend with Kyoto~VR's current setup in
Section~\ref{sec:deployingTheEditourBackend}.

In Section~\ref{sec:alternativeArAppConcepts}, we also discuss the alternative
AR app concepts that were explored before development became completely focused
on ARuko.

\clearpage

% now comes the table of contents, list of figures, and list of tables
% these should be single spaced
\begin{singlespace}
	\tableofcontents
	% uncomment this if you want the Table of Contents to have an entry in the
	% table of contents
	% \addcontentsline{toc}{section}{Table of Contents}
	\clearpage

	\listoffigures
	% \addcontentsline{toc}{section}{List of Figures}
	\clearpage

	\listoftables
	% \addcontentsline{toc}{section}{List of Tables}
	\clearpage
\end{singlespace}

% go back to 1.5 spacing and Arabic numbering for the rest of the paper
\pagenumbering{arabic}

\section{Introduction}
\label{sec:introduction}

The field of consumer-oriented augmented reality (AR) technology is just coming
into full swing, and it presents many unique opportunities for a myriad of
fields. This budding technological platform uses digital displays to augment
real-world information with computer-generated data (see
Figure~\ref{fig:arExample}. As the technology progresses, several disparate
industries have found novel uses for it, some becoming extremely
profitable~\autocite{webster2018}. With smartphone manufacturers investing
billions into AR technology~\autocite{mason2016}, it seems poised to be the next
big digital interface.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{ar-example.jpg}
	% Note: you don't actually need the part in square brackets for the
	% caption, but if you omit it the regular caption text will be used for
	% the List of Figures entry.
	\caption[A virtual model of the solar system projected onto the real world
	using AR]{A virtual model of the solar system projected onto the real world
		using augmented reality~\autocite{tedx2016}}
	\label{fig:arExample}
\end{figure}

Tourism and art are among the fields that could gain the most from AR
technology~\autocite{saenz2009, katz2018}. This technology can provide
accessible information to tourists and art viewers that they would not be able
to get any other way. Kinkaku-ji, the ``Golden Pavilion'' of Kyoto (see
Figure~\ref{fig:kinkakuji1}, is the perfect example of a site that would benefit
from AR~\autocite{bornoff2000}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kinkakuji-1.jpg}
	\caption[The Golden Pavilion]{The Golden Pavilion~\autocite{fan2018}}
	\label{fig:kinkakuji1}
\end{figure}

Kinkaku-ji is a UNESCO World Heritage Site~\autocite{unesco} and one of the
most popular tourist destinations in Japan~\autocite{japanguide2019}. The
Golden Pavilion is a magnificent sight to behold, but the gardens and grounds
are the true beauty of Kinkaku-ji. It is a living masterpiece of Zen Buddhist
landscape design, expanding on the natural beauty of Kyoto's forests and
mountains through the practice of borrowed scenery~\autocite{kuitert2002}.

However, much of the historic information at the site is lost on non-Japanese
speaking visitors because it is often only available in Japanese. In addition to
the language barrier, the information provided does not include most of the
history and context necessary to fully understand Kinkaku-ji.

To solve this problem and open up the peerless beauty of the Golden Pavilion to
more people, our team created ARuko and Editour. These two tools comprise a
complete platform for designing and experiencing AR-enhanced tours. Editour
makes it easy to design custom audio tours including audio, text, and images,
while ARuko allows users to seamlessly experience historic sites in an intuitive
and novel way. Many AR apps require specific markers, like the those shown in
Figure~\ref{fig:arMarker}. Too avoid needing to place any physical object that
would disturb the buildings or grounds of a historic site our app does not rely
on these markers. Instead, ARuko recognizes images of objects that are already
at the site, and uses them to display informative AR pop-ups. See
Sections~\ref{sec:translationFeature} and \ref{sec:mapOverlayFeature} for more
details.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{ar-marker.jpg}
	\caption[Augmented reality markers]{Augmented reality
		markers~\autocite{jeriaska2010}}
	\label{fig:arMarker}
\end{figure}

\clearpage

\section{Background}
\label{sec:background}

As smartphones and mobile technology become more prevalent, new forms of
human-computer interaction are becoming mainstream. Smartphones allow for an
unprecedented degree of connectivity with the digital world, but can also serve
as a tool for digitally enhancing the physical world. In this section we explain
the origins and uses of some of this technology.

\subsection{What is Augmented Reality?}
\label{sec:whatIsAugmentedReality}

Augmented reality, or AR, is a type of human-computer interface where
perceptions of the real world are enhanced by computer-generated information.
This differs from virtual reality (VR) in that a VR experience consists
exclusively of virtual information in a virtual environment. In AR, virtual
information is mixed with sensory input from the real
world~\autocite{carmigniani2011}. This can enhance the user's perception of
reality by providing information that would be difficult or impossible to
display through traditional means.

For example, AR can be used to display information about historical events,
places, and objects overlaid onto images of the real
world~\autocite{saenz2009}, in a way that is more intuitive or visually
appealing than a purely virtual environment. This provides the user with useful
information without needing to alter a real historic site.

\subsubsection{Current Augmented Reality Technology}
\label{sec:currentAugmentedRealityTechnology}

While preparing for this project our team researched the current state of AR
technology. Smartphones are the most commonly used AR hardware by
far~\autocite{boland2018}. Typically smartphone AR applications make use of the
phone's camera, accelerometer, gyroscope, and GPS sensors to reproduce a view
of the real world with virtual information layered on top of
it~\autocite{bonsor2018}.

\subsubsection{Augmented Reality Use Cases}
\label{sec:augmentedRealityUseCases}

Even though it is still a developing technology, AR has been used in many
disparate disciplines, including data visualization~\autocite{resnick2017},
commerce~\autocite{matney2018}, marketing~\autocite{sharma2015},
education~\autocite{stewart-smith2012}, visual art~\autocite{katz2018}, and
even archaeology~\autocite{eve2012}.

The video game industry has readily embraced augmented reality, leading to the
development of many AR games for smartphones and dedicated head-mounted
displays. Perhaps the most popular AR game, \textit{Pokémon Go}, has been
downloaded over a billion times~\autocite{webster2018}. This game makes use of
GPS and camera data to overlay game objects (in this case fictional monsters) on
top of images of the real world~\autocite{concepcion2016} (see
Figure~\ref{fig:pokemonGo}).

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{pokemon-go.jpg}
	\caption[A screenshot from the AR game \textit{Pokémon Go}]{A screenshot
		from the AR game \textit{Pokémon Go}~\autocite{vastateparks2016}.}
	\label{fig:pokemonGo}
\end{figure}

\subsubsection{Challenges of Augmented Reality}
\label{sec:challengesOfAugmentedReality}

Most existing applications using AR, in our team's opinions, do not leverage the
full potential of this emerging technology. As with any new field, our
understanding of AR and how to develop for it is limited. All sorts of different
challenges and limitations face the development and production of AR apps. In
this section we discuss some of the challenges we faced when working on our app.

Many of the applications and use cases that exist for AR right now are, simply
put, not very good. Many AR apps and features available on smartphones offer
very little in the way of functionality or practicality; quite simply, they
primarily revolve around deploying gimmicks~\autocite{theappsolutions2018}. For
example, one of the most popular apps to use AR, \textit{Pokémon Go}, primarily
uses its AR feature to place a Pokémon on top of a camera's video feed.
Disappointingly, it does not interact with anything around it. It just floats a
fixed distance away from the user.

Additionally, in many cases where AR is used, the task could be accomplished as
well or better without AR~\autocite{theappsolutions2018}. Returning to the
\textit{Pokémon Go} example, when the AR feature is left on while capturing a
Pokémon, the extra power needed for this feature drains the device's battery
faster. Also, it makes it harder to hit the Pokémon with a Pokéball due to the
model drifting in the environment. On top of all of this, the performance of
the app suffers drastically, especially on older devices. All of these problems
detract from the ability to play and enjoy the game.

One of the major problems facing AR development as a whole---including our
project---is limited hardware. For starters, smartphone cameras present a
major limitation. Many smartphone cameras only capture images in 2D, which can
make generating AR content in a 3D world difficult without the use of QR or
barcode markers~\autocite{geospatialworld2018}. Additionally, GPS sensors on
smartphones can also be too imprecise for good AR
tracking~\autocite{geospatialworld2018}.

AR apps are most commonly designed for smartphones due to their relatively low
cost and how accessible they are. However, smartphones come with a host of
hardware limitations that make achieving satisfying AR capabilities difficult.
Additionally, AR apps for smartphones tend not to be very user-friendly, and
oftentimes even complicate the task or activity they were meant to
enhance~\autocite{theappsolutions2018}.  The alternative then is to turn to
dedicated hardware, like Microsoft's HoloLens. The issue is that dedicated
hardware like the HoloLens is very inaccessible and expensive; Microsoft only
offers developer editions of the HoloLens, and for the incredibly steep price
of \$3,500~\autocite{microsoft2019}.

The challenges we faced during development mirrored those that plague the
industry as a whole very accurately. We struggled with how exactly we would use
AR for our app in a way that benefited the experience as a whole. Despite our
initial apprehension, we believe the way we have integrated AR into our app
enhances the user experience. Hardware issues were definitely our biggest
challenge. Only one member of our team had a phone new enough to run AR
technology at a decent level, making testing of our application difficult.

\subsection{izi.TRAVEL}
\label{sec:iziTravel}

Before our project started, Kyoto~VR was already using an app to deliver audio
tours. The app, called izi.TRAVEL, allows users to create audio tours with
audio and image files tied to specific locations in the real
world~\autocite{izitravel2015} (see Figure \ref{fig:iziTravel}). The app is
aimed at museums and city tours~\autocite{izitravel} but can be used for tours
of historic sites too. Kyoto~VR used the platform to make a simple audio tour
of Kinkaku-ji.

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{izi-travel.png}
	\caption{A screenshot of the izi.TRAVEL app interface}
	\label{fig:iziTravel}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kinkakuji.jpg}
	\caption[The Golden Pavilion of Kinkaku-ji, Kyoto, Japan]{The Golden
		Pavilion of Kinkaku-ji, Kyoto, Japan~\autocite{davidson2005}}
	\label{fig:kinkakuji}
\end{figure}

Our group aimed to implement all the functionality of izi.TRAVEL and more in
our own app. See Section \ref{sec:aruko} for more details.

\subsection{Augmented Reality Platforms}
\label{sec:platforms}

Before we could start working on creating our app, we first needed to decide
which AR development platform to use. Based on our preliminary research, we
learned there are many powerful platforms available. There were many things we
needed to consider before deciding on a platform. These considerations included
what features the platform offered, which devices and operating systems the app
could run on, and the cost. Detailed below are some of the different platforms
we considered.

\subsubsection{ARCore and ARKit}
\label{sec:ARCoreAndARKit}

ARKit and ARCore are both frameworks that enable apps to take advantage of
powerful AR related features. ARKit is developed by Apple and can only be used
by iPhones~\autocite{summerson2018}. Google's ARCore framework can deploy to both
Android and iPhone. Many cross platform solutions will leverage the full
potential of their target platform by using either ARKit or ARCore ``under the
hood.'' Both ViroReact and Vuforia use ARKit or ARCore depending on the target
platform \autocites{vuforiaFusion}{moon2018}.

ARKit 3 aims to support a wide array of new features and will be available in
iOS 13~\autocite{apple2019}. Notably, one of these features is ``people
occlusion,'' which will allow virtual objects to be realistically obscured when
someone walks in front of the object. This kind of technology could be useful
for us since our app uses AR in crowded places.  As of August 2019, this
feature is not on the horizon for ARCore.

Both ARKit and ARCore have well-supported APIs for Unity. ARKit 3 support is
coming soon to Unity, along with all of the advanced features that it
brings~\autocite{stinson2019}.

\subsubsection{Wikitude}
\label{sec:wikitude}

Wikitude is another major AR development kit that is very prevalent in the
world of AR development. It has over 130,000 registered AR developers, and is
powering over 30,000 AR apps across smartphones, tablets, and smart
glasses~\autocite{wikitude2018}. It is easy to understand why Wikitude has so
many registered developers considering the list of features the platform
offers. Some of the features that appealed most to us were their ``Instant
Tracking,'' allowing for augmentation using flat surfaces; ``Image Tracking,''
allowing for augmentation using one or multiple 2D images; and ``Geo AR,''
allowing for creation of geo-markers at specific locations to trigger AR
content.

Wikitude supports Android, Apple, and Windows phones and tablets, as well as
smart glasses like the Epson Moverio, Microsoft HoloLens, and Vuzix smart
glasses. Additionally, many different development frameworks are capable of
running Wikitude's software development kit (SDK). According to Wikitude's
website, their SDK works with Windows, Android, and iOS development frameworks,
as well as ARCore and ARKit, Flutter, Cordova, Xamarin, and even Unity.

Based on this information, Wikitude was initially appealing to us. However, the
cost was a major issue. Wikitude is free for eligible startups, but given the
criteria they list on their site, we were not confident that we would qualify.
Aside from this option, Wikitude offers a 30-day demo license for \euro{499},
which was far beyond the resources available to us. They also charge
\euro{1990} for their SDK Pro, and \euro{2490} for their SDK Pro 3D. Given
these prices, and our very limited budget, we decided to consider other
options.

\subsubsection{motive.io}
\label{sec:motive.io}

The company motive.io previously was developing a product that would allow
users to create an engaging AR experience in the style of
\textit{Pokémon~Go}~\autocite{odom2017}. Their service would have purportedly
handled technical details such as hosting, storage, and user accounts. The
promotional video shows the creation of a location-based AR game where the user
pretends to hack into ATMs using the real-world locations of ATMs in a city.

Unfortunately, it appears that the company has completely pivoted off of this
idea, and has since moved to developing AR training
software~\autocite{motiveio}. We reached out to motive.io via email for any
advice they might be willing to give us, but we received no response.

\subsubsection{ViroAR}
\label{sec:viroAR}

Viro AR comes in two versions: ViroReact and ViroCore\autocite{facebook2019}.
ViroCore allows developers to build an AR Application with Java. The
disadvantage of this is that ViroCore only allows developers to target Android,
which unfortunately made ViroCore not an option for our project. ViroReact is
the cross-platform option for ViroAR. ViroReact leverages the cross-platform
capabilities of React, which is Facebook's JavaScript Library for developing
user interfaces~\autocite{facebook2019}.

Since our project is not a game, we initially wanted to avoid using a
fully-featured game engine such as Unity (see Section~\ref{sec:unity}.
Therefore, we decided to take the time to explore this framework since it
appeared to be more tailored to our specific use case. Viro Media also
recognizes that all developers who wish to create an app with 3D capabilities
are not necessarily game developers. Viro Media pitches ViroAR as ``The
perfect alternative to specialized game engines, ViroAR is a platform for
rapidly building ARKit and ARCore apps. Our platform allows developers to focus
on what they do best by leveraging familiar tools and frameworks used in mobile
application development''~\autocite{viro2019}. While the majority of our team
has decent experience with writing JavaScript, we have have very little experience
writing React apps.

In most app development scenarios, to test the app on actual hardware, it must
be compiled using either Xcode or Android Studio depending on your platform.
Once you have built the app, your device needs to install the app. This process
can be very time consuming, especially when needing to run frequent tests on
the target platform.

An alternative to building an app for a physical phone is to run it through an
emulator on a computer.

According to Techopedia, ``Emulation is the process of imitating a
hardware/software program/platform on another program or platform. This makes
it possible to run programs on systems not designed for
them''~\autocite{techopedia2019}. Emulators are often very useful for speeding
up development. Loading programs onto the Android emulator is usually much
faster than installing an Android package file (APK) onto the actual device.
Additionally, emulators can mock almost all of the important features of an
actual Android device, including calls, text, GPS position, device rotation and
more~\autocite{androidemulator}.

Since an emulator uses software to emulate hardware, this poses two technical
issues that prevent emulators from being a proper replacement for testing on
real hardware. Emulation is often imperfect, so glitches sometimes emerge in
emulation that do not manifest on the actual hardware~\autocite{alzaylaee2017}.
Conversely, errors that exist in the hardware version may not appear in the
emulation. Secondly, and most importantly for AR, it is common for emulation to
be much slower compared to real hardware.

In a question submitted to \textit{Compute!} magazine asking whether it is
possible for a Commodore 64 to emulate MS-DOS, the editor responded ``Yes, it's
possible for a 64 to emulate an IBM PC, in the same sense that it's possible to
bail out Lake Michigan with a teaspoon.'' The editor goes on to explain
``Emulation is a complex business, but here's one rule of thumb: The only way
to successfully emulate a machine is with a much more powerful
machine''~\autocite{warick1988}. In our case, emulating Android on a desktop
operating system is closer to emulating a C64 on an IBM PC running MS-DOS
(rather than the other way around, thankfully). The takeaway is that the speed
problem facing emulators is just as real as it was in 1988. Considering that AR
already pushes modern phone hardware to the limit, emulation is not something we
wanted to struggle with. On top of this, our testing required us to move the
camera through the world at various speeds and angles, which is simply
cumbersome with a laptop webcam.

As an alternative to emulation, ViroReact offers a convenient ``test bed'' for
rapidly testing your ViroReact app on native hardware without the need for an
emulator. This allowed us to get a real AR app up and running on our devices
much more easily than any of the other frameworks/engines we explored.
Unfortunately, the test bed app was somewhat unreliable based on our
experience; the test bed would frequently crash or would not be able to
download the files from the server. The most reliable way to test our app was
to compile a binary and manually install it onto our devices, which completely
defeated the purpose of the test bed app.

\subsubsection{Unity}
\label{sec:unity}

Unity is a game engine that was originally introduced in
2005~\autocite{axon2016}. Because of the low barrier to entry---knowledge-wise
and price-wise---the engine quickly became incredibly popular. Ironically,
because the engine is so accessible that, Unity has faced a degree of critism.
Because Unity appeals to so many beginners, there are many unpolished games that
bear the Unity logo. In response to this sentiment, Marcos Sanchez, head of
global communications at Unity, "Democratizing development---in our minds, we
don't think is the wrong thing, we think it's the right thing. You want more
people understanding, and everyone’s gotta start somewhere"~\autocite{axon2016}.
Unity CEO John Riccitiello made the claim at TechCrunch Disrupt San Francisco
that half of all games are made with Unity \autocite{dillet2018}. We consider
Unity's ubiquity to be an advantage; there is a glut of information about how to
use augmented reality within Unity.

Even though Unity is beginner-friendly for making games, the augmented reality
app we planned to create was not a game. Therefore, we were somewhat concerned
that Unity's ease-of-use would be undermined simply because we were not making a
traditional game experience. Instead we would be using Unity to create a piece
of software that both looked and felt like a familiar mobile app. Unity has very
limited support for this compared to technologies that are specifically geared
toward creating modern mobile user interfaces such as React.

All of Unity's scripting is done with C\#, which is a general purpose,
object-oriented language~\autocite{ecmainternational2017}. Even though none of
us had extensive experience with C\# prior to this project, this was not a great
concern since C\# is similar to other object-oriented languages such as Java and
C++. The design goals of the language were also appealing to us; C\# includes
features important for programmer productivity such as strong type checking
(unlike JavaScript), detection of using uninitialized variables (also unlike
JavaScript,) range checking for array bounds (unlike C) and garbage collection.
At the same time, our lack of familiarity with the language and Unity as a whole
is another reason we first looked into cross-platform AR solutions that did not
use Unity.
%TODO see if we need more sources for this

At the same time, Unity has excellent support for augmented reality through
various extensions. Because we initially understood AR to be the core of the app
experience, we chose to use Unity with the Vuforia extension for all future
development on the mobile app.

Unity is distinct from the technologies previously discussed in this section
because it is not an AR platform unto itself. Instead, Unity can take advantage
of AR platforms such as ARKit, ARCore, Vuforia and Wikitude through the use of
plugins.
%TODO provide citations that note that all of these platforms support Unity.

\subsection{Kinkaku-ji}
\label{sec:kinkaku-ji}

Kinkaku-ji, the Temple of the Golden Pavilion, is undoubtedly one of Japan's
greatest attractions.  As of September 2019, it was the second-most visited site
in Kyoto---second only to Kyoto Station \autocite{japanguide2019}. Kinkaku-ji's
rich history and stunning beauty bring tourists from all over the world to see
its shining gold walls and magnificent gardens. Our project intends to enhance
users' enjoyment and understanding of Kinkaku-ji through informative interactive
media, teaching about the history and significance of the site.

The site that is now Kinkaku-ji was originally a villa owned by poet and
nobleman Saionji Kintsune, until the land was bought by shōgun Ashikaga
Yoshimitsu in 1397~\autocite{noboru2013}. As Zen Buddhism was in vogue among the
warrior class in Japan at the time, the site was converted into a Zen temple
according to Yoshimitsu's wishes upon his death~\autocite{bornoff2000}.

The main pavilion is a three-story tall building on the edge of a landscaped
pond. The outer walls of the top two floors are completely covered in gold leaf,
leading to the name ``Golden Pavilion'' (\begin{CJK}{UTF8}{min}金閣\end{CJK}
\textit{kinkaku}). The grounds surrounding the Golden Pavilion are in the style
of a Japanese strolling garden, with carefully manicured plants and paths and
designated viewing points. The buildings and landscaping make use of a Zen
practice called ``borrowed scenery'' (\begin{CJK}{UTF8}{min}借景\end{CJK}
\textit{shakkei}), in which the background landscape is incorporated into the
design of a garden~\autocite{noboru2013}.

The pavilion sits on the edge of a carefully designed pond (see Figure
\ref{fig:kinkakujiGrounds}), with small islands designed to replicate the shape
of Japan and several other famous locations from Japanese and Chinese
literature~\autocite{young2007}. All of these landscape features have been
immaculately maintained and preserved through the Golden Pavilion's six hundred
year history.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kinkakuji-grounds.jpg}
    \caption[Expertly landscaped islands in the Kinkaku-ji mirror pond]{Expertly
		landscaped islands in the Kinkaku-ji mirror pond~\autocite{desousa2014}}
	\label{fig:kinkakujiGrounds}
\end{figure}

In 1950 the pavilion was burned down by a paranoid schizophrenic monk who had
been living at the temple~\autocite{borowitz2005}. The structure was rebuilt in
1955, close to the original design but with more extensive gold
leaf~\autocite{bornoff2000}. The interior was also restored later to match the
original.

None of this history is readily apparent to a typical non-Japanese speaking
tourist at Kinkaku-ji. There are a few signs describing different areas, but
they are only available in Japanese and contain very little information about
the history and design of the site. Because of this, an intuitive audio tour app
with AR features would be very helpful to tourists at Kinkaku-ji.

\clearpage

\section{Implementation and Technology}
\label{sec:implementationAndTechnology}

In this section we discuss and implementation of ARuko and Editour, including
the technologies and techniques involved in creating both of them. We also
discuss the design decisions we made and some of the challenges we faced in
creating these applications.

\subsection{Editour: The Tour Editor}
\label{sec:editour}

The goal of Editour is to allow a non-technical user to design audio tours
through a web app. At its core, Editour is a tool to draw arbitrarily-shaped
geographic regions onto a map, edit those regions and assign media to each of
the drawn regions.

An audio file and multiple image files can be uploaded for each region the user
defines. The user can also provide a text transcript of the audio file as well.
After naming and uploading the tour with the ``Upload'' button, our backend
builds a zip containing files that can be placed directly into the Unity
project.

Our backend allows users to save their tours to a server and load those same
tours back into the editor to resume work. The geographic area of each region
can be adjusted by clicking and dragging. Vertices can also be added and
deleted if the needs of the tour change over time.

\subsubsection{The Need for an Editour}
\label{sec:theNeedForAnEditour}

One of the goals for our project is to create a working prototype that others
can build off of in the future. Similar to the app izi.TRAVEL discussed in
Section~\ref{sec:iziTravel}, we needed some way to easily define arbitrary
polygons (which we call ``regions'') and associate media with each one, such as
audio and image files.

For testing purposes, we initially defined these regions directly in the C\#
scripts, hard-coding the coordinates. Below is an example of a quadrilateral
surrounding the Creation Core building at the Ritsumeikan Biwako-Kusatsu campus.

\begin{minted}{csharp}
Regions.add(new GPSPolygon(new List<GPSPoint>{
    new GPSPoint(34.979222, 135.963628),
    new GPSPoint(34.979187, 135.965130),
    new GPSPoint(34.979794, 135.965053),
    new GPSPoint(34.979754, 135.963669)
}, "Creation Core"));
\end{minted}

Even with knowledge of C\# and Unity, inputting tour data this way is obviously
cumbersome. Atticus is not a software developer, and does not want to work with
C\# scripts. In order to make our app prototype at all useful for the future,
we needed to provide an easy way to ``design'' a tour using a simple graphical
interface.

\subsubsection{A Complete Tour Definition}
\label{sec:tourDefinition}

In order to create a fully-functional editor for designing and exporting
``tours,'' we had to consider how to fully define a tour purely in terms
of text and a directory of associated media.

Here is an example of a tour metadata file in JavaScript Object Notation (JSON)
format:

\begin{minted}{json}
{
   "regions": [
      {
         "name": "Beautiful Place",
         "points": [ {"lat": 34.979222, "lng": 135.963628}, ... ],
         "audio": [ "beautiful-audio.mp3" ],
         "images": [ "mountain.jpg", "stream.jpg" ],
         "transcript": "To your left, you can see a beautiful place."
      },
      {
         "name": "Gorgeous Place",
         "points": [ {"lat": 34.979754, "lng": 135.964889}, ... ],
         "audio": [ "gorgeous-audio.mp3" ],
         "images": [ "hill.jpg", "valley.jpg", "gorge.jpg" ],
         "transcript": "To your right, you can see a gorgeous place."
      }
   ]
}
\end{minted}

Within each object in the region list, we can see that the region has five
properties: a name, a list of coordinates, a list of audio files,\footnote{In
the current design of the app, it only makes sense to associate one audio file
per region, but the format makes it easy to accommodate for multiple audio
files in the future.} a list of multiple image files, and a transcript of the
audio file. One disadvantage is that all media files have to have unique names.
This is because the zip that is created by the Editour backend contains all of
the media files in a flat file structure. The above JSON is used to associate
each media file with a region, meaning that the filename needs to act as a
unique identifier. The backend will respond with a message if it finds
duplicate filenames, prompting the user to use unique names for all files.

\subsubsection{Editing a Tour}
\label{sec:editingATour}

The user can easily create a new region by shift-clicking the map, and then
continuing to click to define more points. Figure~\ref{fig:drawingRegion} shows
this in action. To terminate the polygon, the user must shift-click again. The
``Welcome to Editour'' card (seen in Figure~\ref{fig:welcomeCard}) provides
these instructions, which should be enough to get started. A user can only
terminate the polygon after two points have been placed to prevent the user
from drawing a region with an area of zero. Once a region is created, a region
card appears in the side bar. From here, the user can edit anything about the
region.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{welcome-card-editour.png}
    \caption{The ``Welcome to Editour'' card providing basic instructions to
    get started}
	\label{fig:welcomeCard}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{drawing-region-editour.png}
	\caption{Editour when a region is being drawn, including the two dotted
		preview lines}
	\label{fig:drawingRegion}
\end{figure}

The top of the region card displays the region name in bold text. By clicking
on the region name, the map will pan and zoom to frame the region in the center
of the screen. This is useful if there are many regions. To the right of the
region name, there are two arrow buttons, one pointed up and another pointed
down. This allows the user to reorder the regions in the column. This was one
of the features that was added late into development in order to adapt to an
important change in the tour app, ARuko. This is discussed in
Section~\ref{sec:adobeXdDesign}.

In the region card, there are four multicolored expandable ``subcards'' that
allow the user to edit different information about each region. Since there can
be many region cards in the column, it was important to be able to collapse
each section. Collapsing the subcards gives the user enough room to see
multiple region cards at a time.

The first subcard seen in Figure~\ref{fig:renameSubcard} is the ``Rename''
subcard, which is fairly self-explanatory.  This section expands to reveal a
textbox, allowing the user to rename the region after clicking ``Okay'' or
hitting the enter key.\footnote{There are multiple places in the UI where a
textbox is immediately adjacent to a ``Confirm'' button.  We made sure all of
these can be triggered by simply hitting enter, since this is common behavior
when filling out forms on the web.}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{rename-subcard-editour.png}
	\caption{The ``Rename'' subcard}
	\label{fig:renameSubcard}
\end{figure}

The second subcard seen in Figure~\ref{fig:mediaSubcard} is slightly more
involved. Expanding the ``Media'' subcard by clicking on the green bar will
reveal a bevy of options for augmenting the audio, images, and text to be
associated with that region of the tour. From here, the user can upload an
audio file and multiple image files from his or her hard drive with the two
file-select fields. If the user is editing an existing tour that has already
been uploaded, the names of the files already present on the server will appear
as cards with `X' buttons. This allows the user to remove images or audio from
that region even after the tour has been uploaded.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{media-subcard-editour.png}
	\caption{The ``Media'' subcard}
	\label{fig:mediaSubcard}
\end{figure}

The third subcard is the ``Points'' subcard. This subcard can be revealed in
one of two ways. The first way is by clicking on the yellow bar labeled
``Points'', which is the same for all subcards. Alternatively, the user can
click directly on the region on the map. The region will flash yellow and be
scrolled into view to let the user know which region was clicked on. Both of
these methods will enable editing mode, whereby the user can directly
manipulate the vertices that make up the region. Blue circles appear on top of
each vertex, and red circles appear at the midpoint of each edge. Clicking on a
red midpoint will add an additional vertex at that midpoint. Dragging one of
the blue vertices will move the vertex around. The latitude and longitude are
updated in the ``Points'' subcard as the user drags the vertex around. Clicking
on the vertex directly will create a blue pin-shaped marker that the user can
also drag. Clicking on the coordinate box in the subcard will also create this
blue pin-shaped marker; the map will also pan to this coordinate. The user can
also delete vertices in the same way that files can be deleted from the
``Media'' subcard by clicking on the `X' button on that vertex's coordinate
card. If there are exactly three points in a region, the `X' buttons will be
``grayed-out,'' preventing the user from deleting any more points, since a
polygon must consist of at least three points.

\begin{figure}[h]
	\centering
    \includegraphics[width=\textwidth]{edit-subcard-with-map-editour.png}
    \caption{The ``Points'' subcard with the map showing vertices and edges that
    can be edited}
	\label{fig:editSubcardWithMap}
\end{figure}

The third subcard pictured in Figure~\ref{fig:deleteSubcard} is the ``Delete''
subcard. When revealed, another button will appear labeled ``Really Delete.''
The action can be canceled by collapsing the subcard by hitting the bar now
labeled ``Don't Delete!'' This prevents the user from accidentally deleting a
region he or she did not mean to remove.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.4\textwidth]{delete-subcard-editour.png}
    \caption{The ``Delete'' subcard}
	\label{fig:deleteSubcard}
\end{figure}

Above all of the region cards, there are various options for uploading,
downloading, renaming and deleting tour files on the server. Within the upload,
download and delete cards there are message boxes to indicate the status of
that operation. When uploading and downloading, a percentage will be displayed
indicating the upload/download process, which can be seen in
Figure~\ref{fig:uploadingMessage}. If an HTTP 200 (OK) or 201 (Created)
code~\autocite{rfc7231} is returned from the server, a ``Success'' message will
appear. If there was some problem, the error message returned by the server is
displayed in this box instead. Since uploading, downloading, and deleting are
all network-related operations, it makes sense to include this message box for
each card.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.4\textwidth]{uploading-message-editour.png}
    \caption{The upload subcard with an upload status}
	\label{fig:uploadingMessage}
\end{figure}

When the page is loaded, the frontend will request the names of the tour files
stored on the server from the backend. This data is used to create a box of
buttons, which includes one button for each tour file on the server. Clicking
on one of these blue buttons will fill the textbox in with the correct name.
Before this feature was implemented, the user had to correctly type the name of
the saved tour into the textbox in order to retrieve it. The user is still free
to do this, but the buttons make this easier and less error-prone. This feature
can be seen in figure Figure~\ref{fig:downloadCard}. Clicking ``Download'' does
not download the tour to the user's computer. Instead, it requests only the
tour metadata from the server so the tour's data can be displayed in the web
app. The user can then edit the tour and upload a new version.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.4\textwidth]{download-card-editour.png}
    \caption{The download card}
	\label{fig:downloadCard}
\end{figure}

We did not include a way to resolve addresses or place names, like the service
provided by Google Maps. Instead of exploring plugins/libraries that might
provide this functionality, we offered a simpler solution due to time
constraints.  You can type in a latitude and longitude directly and hit the
``Jump'' button to pan to the specified location. We also hard-coded a few
shortcuts for places that Atticus might want to design a tour for in the future,
which can be accessed by clicking on the yellow buttons with place names.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.4\textwidth]{jump-card-editour.png}
    \caption{The jump card}
	\label{fig:jumpCard}
\end{figure}

Once a tour is either uploaded or downloaded, a link to download the zip file of
that tour will appear. The zip file contains all of the uploaded media, and a
metadata file. The metadata is in the form of a JSON file; it contains all of
the latitudes, longitudes, and enough information to associate all of the media
with the correct regions. The downloaded zip file is what we decompress and
place into the Unity project before it is compiled. This feature is also useful
if the user wants to check the audio and image files that were uploaded.

%update this figure
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{editour.jpg}
	\caption{A screenshot of the Editour web app frontend}
	\label{fig:editour}
\end{figure}

\subsubsection{Editour Frontend Implementation}
\label{sec:editourFrontendImplementation}

The frontend was implemented as a web application, using web technologies such
as HTML, CSS and JavaScript, with JavaScript being the vast majority of the
code. Type-checking provided by Visual Studio Code and JSDoc made the growing
code complexity more manageable.\footnote{One helpful specification of the
ECMAScript standard in the way of programmer sanity is ``strict mode.'' By
placing the string \texttt{"use strict"} at the top of a JavaScript source file,
errors will be thrown in many cases where ordinary JavaScript would not
complain. The simplest example of this is the line \texttt{a = 1;} will not
throw an error ordinarily. In strict mode, you must at first explicitly declare
the variable with the \texttt{let}, \texttt{var}, or \texttt{const} keyword
before assigning it, similar to Java or C++. We included the string at the
beginning of every source file except for class declarations, which are in
strict mode by default~\autocite[386]{ecmainternational2019}.} Excluding Node.js
modules, the only external library we used for Editour was Leaflet, which is
``an open-source JavaScript library for mobile-friendly interactive maps''
according to the official Leaflet website~\autocite{leafletjs}. We did not use
any framework for developing frontends like Facebook's React or Google's
Angular. Instead, all of the complex UI interactions were handled by modifying
the DOM directly with JavaScript's built-in functionality. Furthermore, we did
not use any advanced frontend build tools such as Rollup, webpack or Parcel. For
a JavaScript project of this scope, this was the correct decision. Considering
we had limited knowledge of React or Angular, it was also faster to develop the
UI in the way we were used to from previous experience. But if we were to
continue to develop Editour, UI libraries and frontend build tools would be
something to consider.

\subsubsection{Editour Backend}
\label{sec:editourBackend}

The server-side backend of the Editour application is written in JavaScript for
Node.js. This backend both serves the static frontend content and runs
server-side scripts that process incoming application programming interface
(API) requests. We chose Node.js as the server-side framework because of our
prior experience with JavaScript, its high-performing asynchronous
architecture~\autocite{orsini2013}, and its ease of development.

When a user submits new tour from the web application, the files and metadata
are sent to the server running the Node.js scripts. The server zips up the files
and saves them to the disk with a timestamp. Then any application can request a
particular tour and the backend will serve the most recent version.

As previously mentioned in Section~\ref{sec:editingATour}, our web application
allows users to edit existing tours. When the user requests to edit a tour on
the frontend it sends a request to the backend server for that tour's metadata
file. This file contains information about all regions and files in the tour.
The user can then edit the regions, change names, upload new files, or delete
existing ones without ever having to download the other tour files from the
server, which could take a long time depending on their size. When the user is
done editing they can upload the new metadata and any newly added files to the
server, which intelligently collects new and old files required by the tour and
zips them into a new tour file on the disk.

The backend implements a representational state transfer (REST) API.
In a ``RESTful'' service, web resources are manipulated with a predefined set of
stateless operations~\autocite{rfc7231}. Because the REST API is operated over
the Hypertext Transfer Protocol (HTTP), the available operations are just the
standard HTTP methods, such as GET, POST, and DELETE. For full documentation of
the backend's API, see Appendix~\ref{sec:editourAPIDocumentation}.

\subsubsection{Deploying the Editour Backend}
\label{sec:deployingTheEditourBackend}

For testing and development purposes the Editour backend was originally hosted
on Joseph Petitti's personal server. This was a CentOS virtual machine running
on an old Dell rack server in Worcester, Massachusetts. We wrote a custom
systemd service to run the Editour as a daemon and keep its log files organized.
The Node.js backend was served by an Nginx reverse proxy running on the same
virtual machine.

While this worked well enough for testing, it was not a permanent solution. We
asked Atticus to provide a more powerful---and geographically closer---hosting
solution for running the Editour in the future. He obtained a domain name and a
shared hosting environment on a Red Hat Enterprise Linux (RHEL) server for us to
use on September 23\textsuperscript{rd}. Unfortunately, this shared hosting
environment was designed for running simple PHP applications like WordPress, so
it took a bit of work to coerce it into running a Node.js application.

We were able to get shell access to the shared hosting environment, but without
superuser privileges. This meant we could not install packages, change system
settings, or access \texttt{systemctl}, the command that manages systemd
services. In order to install Node.js and npm in this environment we had to use
Node Version Manager, a series of shell scripts that handles downloading and
compiling various versions of Node~\autocite{nvmsh2019}.  The only difficulty
with this was finding a version of Node.js that could compile with the older
version of GCC that the RHEL server had. After resolving this challenge, we were
able to simply clone the backend from GitHub and run it with npm.

However, the server already had Apache listening on port 80, and without
superuser privileges we could not edit the Apache configuration files to change
this. Instead we decided to configure Apache as a reverse proxy, using only the
rules that can be put in a \texttt{.htaccess} file,\footnote{This is a
directory-level configuration file with limited options compared to Apache's
global configuration files.} since this is the only configuration file we could
access. Using Apache's \texttt{mod\_rewrite} module we were able to rewrite
incoming requests on port 80 to instead go to \texttt{localhost:3000}, the port
where Node.js was listening~\autocite{apache2019}.

With no access to \texttt{systemctl} we could not use the custom systemd service
we wrote to manage the server daemon. Instead, we used a custom shell script and
an npm package called ``forever'' to handle starting and stopping it. The
forever package runs Node.js scripts continuously, automatically restarting them
if they crash~\autocite{robbins2019}.

Tours are stored on the server as compressed zip files, but when uploading or
editing a tour these zips are decompressed to a temporary directory. To prevent
the server's limited disk space from filling up with old temporary files we
wrote a simple bash script that removes directories that have not been modified
in the past five days. We set up a cron job\footnote{The program cron allows for
periodic scheduling of repeated commands} to run this script automatically once
per day to clean out old temporary files that are not being used any more.

Unfortunately, the hosting solution Kyoto~VR provided us with severely throttles
disk write speeds. Because the backend has to write potentially large amounts of
data to the disk every time a tour is uploaded or edited this becomes a major
bottleneck for the web app's performance. For large tours (bigger than 50 MiB)
this can cause an issue where the Node server takes so long to respond that
Apache thinks it has timed out, and sends an erroneous 504 Gateway Timeout
message~\autocite{rfc7231}. Without access to superuser permissions or a faster
hosting provider there is nothing we can do about this bug. The Node server does
eventually complete the request and respond correctly, so until Kyoto~VR can get
a better hosting provider we consider it a minor issue.

\subsubsection{Future of Editour}
\label{sec:futureOfEditour}

In the current implementation, the tour file generated by the Editour backend
still has to be moved manually to the Unity project folder. In the future, we
would like the app to be able to contact a server running a version of the
Editour backend and update the tour info by itself. This would allow Kyoto~VR to
update the tours without updating the entire app.

Furthermore, we foresee Editour being a useful tool outside of the app we
designed for Kyoto VR. The design of the Editour is agnostic about how the media
will be used. For an app like izi.TRAVEL, the images are displayed when the
associated audio tour is playing.

The files produced by Editour could be used to power an app that does not
incorporate AR, such as an audio guide mobile web app that runs entirely in the
browser, using the geolocation API to access the user's location (if the user
grants permission.) This kind of infrastructure would allow users to access an
audio tour---supplemented by images and audio transcripts---without the need to
download an app at all. By simply visiting a link, a user could experience a
guided tour that changes as the user walks through the route. For example,
colleges could distribute tour links via QR codes, which would instantly launch
an interactive guided tour of the campus through the browser.

Currently, anyone can clone the GitHub repository of Editour and deploy a
working version on a personal server. The source code for Editour is under a
free license. This allows anyone to expand the frontend and backend to suit
their needs. With additional hardware and by extending the current
implementation of both the frontend and backend, Editour could be used as the
groundwork to create an entire crowd-sourced walking tour platform, allowing
users to design tours for others to take.

In summary, we want to explore the idea of creating a community-driven audio
tour database that allows mobile users to take tours without the need to install
an app. The free code we wrote for Editour empowers us (or anyone else)
to develop and launch this platform in the future.

\subsection{ARuko}
\label{sec:aruko}

% TODO let's triple check the name in Japanese even makes sense
ARuko, named after the Japanese word ``\begin{CJK}{UTF8}{min}歩こ\end{CJK}''
(\textit{aruko}), meaning ``let's walk,'' is the app we created to guide users
through an AR-enhanced tour. The app was built with Unity, which enabled to us
to use a single codebase for the iOS and Android versions of the apps.

\subsubsection{GPS Functionality}
\label{sec:gpsFunctionality}

The core user experience of ARuko is the walking tour. By walking the given
route, the user enters and exits specific geographic regions that invoke
changes in the app, such as playing audio and displaying different images.
Originally, we planned to mark regions by singular points, and trigger that
region if the user was within a certain radius of that point.
Implementation-wise, this would have been much simpler, but obviously limits
possible tour designs. Consequently, we decided at an early stage to
accommodate regions with arbitrary shape and size.  In the implementation, we
provided support for both ``GPS Bubbles'' and ``GPS Polygons.'' In the end
however, Editour made it trivial to use the more versatile ``GPS Polygons,'' so
we eventually dropped support for regions triggered by distance.

\subsubsection{Translation Feature}
\label{sec:translationFeature}

ARuko uses augmented reality to translate text at the site as well. While AR
translation apps exist, the experience can be frustrating, and produce poor
results. Google's text recognition and machine-learning powered translation
technology is certainly impressive. At the same time, the translation service
works better in ``scan'' mode instead of ``instant'' mode. In ``instant'' mode,
the app attempts to translate text as quickly as possible, superimposing the
translated text onto the real world using the camera. %could put an image of instant mode here as an example
In ``scan'' mode, the user first takes a photo. Then, the user is prompted to 
highlight where text appears in the photo. %could put an example here as well
After the translation is complete, the translated text appears in a separate UI %another example?
element instead superimposed onto the photo. In most cases, it is more 
convenient and readable to have the translation appear in a separate scrollable
text box. This motivated the current design of the ``info buttons'' in ARuko. 
Instead of placing text directly onto signage, we place a blue button with a 
stylized `i' in the center. By tapping on this button, a separate UI element 
will appear containing the translated text. Since this button is cast into the %example
world using the AR camera, the button appears to be placed directly onto the 
content in the real world.

\subsubsection{Map Overlay Feature}
\label{sec:mapOverlayFeature}

Near the entrance of Kinkaku-ji before the ticket gate, there is a large
illustrated map. Much like the signage, there is no language on the map other
than Japanese. Using the AR camera within the app, information can be projected
onto the map. Figure~\ref{fig:arMapFeature} shows the blue highlighted path of
the walking tour, including a few translated labels.

\begin{figure}[h] \centering
    \includegraphics[width=0.4\textwidth]{ar-map-feature.png}
    \caption{Map overlay displaying the path and translated labels}
    \label{fig:arMapFeature}
\end{figure}

\subsubsection{Virtual Souvenir Feature}
\label{sec:virtualSouveneir}

Tickets to enter Kinkaku-ji are long sheets of printed paper with calligraphy.
The tickets are large and visually appealing; they are souvenirs unto themselves
that one might feel inclined to keep around. Over the course of our three month
stay we visited Kinkaku-ji multiple times. For each visit, we received identical
tickets. Seemingly, the design on the tickets do not change on a regular basis,
if at all. Figure~\ref{fig:kinkakujiTicket} shows a ticket we kept from one of
our visits to Kinkaku-ji.

\begin{figure}[h] \centering
    \includegraphics[width=0.4\textwidth]{kinkakuji-ticket.jpeg}
    \caption{Kinkaku-ji ticket}
    \label{fig:kinkakujiTicket}
\end{figure}

Because the tickets are unlikely to change frequently, this enables us to
provide the user with a ``virtual souvenir'' using the AR camera. This is the
only AR feature that is not tied directly to an image target that can only be
accessed at Kinkaku-ji. While the image gallery and segments of the audio tour
can be accessed at any time using the ``Chapter Select'' menu, most of the AR
features can only be experienced during the walking tour, pointing the camera at
objects physically located at the site. Augmenting the ticket with 3D content
allows visitors to take something home, giving them a reason to launch the app
later.

We found a free untextured model of the Golden Pavilion to use for this
feature. Using Unity's built-in capability to design materials, we created
a shiny golden material and applied it to the model (see
Figure~\ref{fig:souvenir}.

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{souvenir.png}
	\caption{ARuko's virtual souvenir feature}
	\label{fig:souvenir}
\end{figure}

Originally, the AR model of the Golden Pavilion was flush against the plane of
the ticket, sitting on the paper as if it were ground. Atticus suggested that we
re-orient the model such that the ticket served as a back plane. This would
allow users to hold the ticket vertically, and view the Golden Pavilion floating
in front of the ticket. This allows users to place their hands under the base of
the Golden Pavilion model as if they were holding it; Vuforia's image tracking
is flexible enough such that the image target can be partially obscured by the
hand.

\subsubsection{Early User Interface}
\label{sec:earlyUserInterface}

As ideas changed and ARuko became what it is today, the user interface (UI) also
changed greatly from its early conception to the final version. Still, the early
UI was a valuable learning experience, providing us with skills that were useful
when building the final version.

The early UI was very much a product of ARuko's development; many of its
elements were created on the fly as new features were added. As more features
were added, it became clear that the UI could not consist simply of buttons and
other basic features provided by Unity. We would need to learn more about how to
create more interesting and useful UI elements to accommodate for our new
functionality.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{old-ui.png}
	\caption{A screenshot of our early UI}
	\label{fig:oldUI}
\end{figure}

Pictured in Figure~\ref{fig:oldUI} is our early UI. It was a very minimal UI,
but still covered all of the uses for our app at the time. The top of the screen
had two dropdown menus. The one on the left (which is open shown open in
Figure~\ref{fig:oldUI}) shows AR image targets in the area that users could
look for and scan. The right dropdown would be dynamically populated in each
region with buttons bearing different images. These images were ones that
Atticus wanted to be viewable in AR using ground plane detection. When tapped,
each button would create a 3D Cube, with the given image on each face, and place
it on the ground where the user was pointing. Figure~\ref{fig:floatingCube}
shows this feature in action.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{floating-cube.png}
	\caption{An ``art cube'' floating over the water containing a placeholder
    programming meme}
	\label{fig:floatingCube}
\end{figure}

Atticus initially wanted us to explore the efficacy of using Vuforia's 2D image
target capabilities for scanning objects outside of the expected use case, such
as 3D landscapes or the side of a building. We had serious doubts about how
well this would work, and our suspicions were confirmed after some field
testing. This was the reasoning behind focusing on ground-plane detection
instead of image targets; ground-plane detection can be used anywhere, and is
much more reliable than treating 3D objects as 2D image targets.

Learning to create these dropdown menus proved to be a very important lesson
later on, since we would put that exact knowledge to use multiple times in the
final UI. We were able to create the dynamically populated dropdown menus by
creating an empty vertical layout group. Then, a script would create buttons for
each image to display, and assign those images as children of the layout group.
Then, to get the smooth open/close animation, we ``lerp'' (linearly interpolate)
the menu to the corresponding position each frame until it is fully open or
closed. In between these two dropdown menus is a panel that would display the
name for the region the user is in at the time. The empty space in the middle of
the screen is where the AR camera display would be shown. In this
implementation, the AR functionality was the focus of the app, so it made sense
to display the AR camera feed at all times. Lastly, on the bottom is the audio
slider for the audio file played in each region. We learned about how audio
sliders work in Unity with this feature, and would expand on that knowledge for
the final version of the app in order to optimize the slider's functionality.

Although most elements of the initial UI did not survive into the final product,
it was still quite valuable as a learning tool. While crafting this prototype we
discovered many of the intricacies of UI design in Unity. For example, all UI
elements must be attached to a Unity canvas element, which covers the entire
phone screen. Elements should be anchored to edges or corners of this canvas so
that they stay in a constant location on different-size screens.

\subsubsection{Adobe XD Design}
\label{sec:adobeXdDesign}

By the end of the project, we were able to turn ARuko into an app rich with
features and polish. However, many of these features were requested relatively
late into the project. This section discusses some of our initial concerns when
we were first given the task of bringing Atticus's ideas to life.

On September 10\textsuperscript{th} Atticus Sims presented us with a storyboard
for the app experience. We would not be able to resume work until September
17\textsuperscript{th} due to prior travel plans. The design he put forth had
ramifications that rippled throughout the entire software stack we had
constructed up to this point. Figure~\ref{fig:adobeXdDesign} shows the
experience design document created in Adobe XD.

% TODO split up this figure into four separate figures, change references to
% refer to the correct ones
\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{adobe-xd-design.png}
	\caption[A screenshot of the Adobe XD project designed by Atticus Sims]
    {A screenshot of the Adobe XD project designed by Atticus Sims}
	\label{fig:adobeXdDesign}
\end{figure}

One of the notable changes is the chapter select screen, which is the third
screen in Figure~\ref{fig:adobeXdDesign}. Before, we worked under the assumption
that each geolocation-triggered region would not have to be ordered in any way.
This assumption made its way into Editour, which at the time did not have a way
to reorder regions once they were placed. Because of this chapter selection
screen, we needed some way to specify and rearrange the order of regions within
Editour.

The inclusion of a ``chapter selection'' screen had other notable design
implications as well. Initially different regions were triggered solely by the
GPS location. Walking into a region would cause the available images to change
on screen. Also, a new audio track would begin to play. This forced us to answer
certain basic questions about the chapter select screen for ourselves. For
example, consider a user that steps into one region, triggering the audio and
images. Then, that user selects a chapter from the chapter selection menu,
overriding the ``true'' region the user is physically standing inside of. When
does the GPS ``take over'' again? Should the region change back to the ``true''
region once the audio of the selected region finishes? This might be intuitive
and preferred for most users, but what if the user is more interested in the
image gallery than the audio? Then, the persistence of the image gallery is
strangely tied to the duration of the audio track playing underneath. What if a
user selects a chapter, and then physically enters that region moments later?
Should the audio restart to indicate that the user has arrived at the location,
or will this appear as though the audio track skipped to the beginning again for
no reason? This seemingly minor inclusion created a significant amount of
discussion.

Other concerns about the chapter selection feature were technical. For a long
time, we were working under the assumption that regions would only be triggered
by GPS location. Because of this long-standing assumption, we were uncertain
about what bugs implementing a chapter selection feature would introduce.

Another notable change can be seen in the first screen on
Figure~\ref{fig:adobeXdDesign}. Atticus was not enthusiastic about being able
to place images onto the ground-plane using the AR camera. Instead, he
suggested that we have the images be viewable in a more traditional
fashion---in 2D as part of the UI.

At this point in the project, the codebase for Editour was already around three
thousand lines of vanilla JavaScript. With the changing requirements, Editour
was becoming onerous to maintain at this stage of the project. Luckily, these
changes were still manageable to implement.

With more time, we might have been able to get the ground-plane image placement
feature to a state where Atticus might have liked it. We wanted to explore ways
to theme the 2D images in the 3D world, such as by framing the images in a
traditional Japanese pagoda. We had already spent two weeks on this feature, so
dropping it was a setback we had to plan around.

On the same screen, there is a scrollable window of text for a transcript of
the audio. Because of this, the Editour UI and tour metadata JSON file needed
to be updated to accommodate one transcript per region. On top of this, the C\#
code that interprets the tour file needed to be updated to expect an audio
transcript field.  Figure~\ref{fig:editourAdditions} shows the UI elements that
changed in order to add these new features.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{editour-additions.png}
	\caption{Additions to the Editour to accommodate the new design}
	\label{fig:editourAdditions}
\end{figure}

With the ground-plane detection features stripped away from the main app, the
AR focus shifted to translating the maps and signs using the camera. The fourth
screen on Figure~\ref{fig:adobeXdDesign} shows a mock-up of this feature.
Atticus took photographs of various signs throughout Kinkaku-ji for us to use
as image targets. Hikaru Inoue helped us by translating all of the text in
these photos from Japanese to English. In this design, the AR camera
superimposes an ``info'' button on top of the sign. Tapping on this button
brings up a separate UI element containing the translation, which can be seen
in Figure~\ref{fig:signTranslation}. The details of this feature is discussed
more thoroughly in Section~\ref{sec:translationFeature}.

\begin{figure}[h] \centering
    \includegraphics[width=0.3\textwidth]{sign-translation.png}
    \caption{Translated sign using AR camera}
    \label{fig:signTranslation}
\end{figure}

Since Adobe XD is largely a prototyping and design tool, a considerable amount
of legwork would still need to be done to rebuild the proposed design using
Unity's UI components. Adobe XD allowed us to export a few important UI
elements as PNGs, such as the buttons to enter AR mode and view the map. We
briefly explored a few solutions to automating the task of getting the Adobe XD
project into Unity. We found one paid extension (for \$18) on the Unity Asset
store called ``Experience Importer - Adobe Xd files importer.'' The download
page for this extension claims ``Transfer AdobeXd project directly to Unity.
Forget about hours of positioning objects. They're already there! Just drag .xd
file to project and...  voila!''~\autocite{glasseye2019}.  We suspect this
sounds too good to be true because it is. As of September
20\textsuperscript{th}, 2019, there are 6 user reviews. All of the five-star
reviews were have the caveat ``Reviewer was gifted package by publisher'' which
which we interpreted as a warning sign. Because of this, we decided to rebuild
the UI largely from the ground up, using the Adobe XD file to guide us.
%TODO refer to the section that discusses building the ui

\subsubsection{Final UI Implementation}
\label{sec:finalUI}

The lessons we learned from the initial UI design (see
Section~\ref{sec:earlyUserInterface}) helped us create the final design in such
a short period of time. Although the new UI is significantly more complicated
than the original prototype, we were able to implement all of Atticus's desired
features within the time limit.

One of the first hurdles we had to overcome was keeping certain UI elements
constant while the user switches between the home screen, AR camera, and map
view. The mode buttons, audio controls, and top bar need to remain on the screen
at all times. To achieve this, we divided the screen into panels. Only one of
the three main panels are set to ``active'' at a given time. Previously, the app
left the AR camera running at all times, since there was only one main screen.
In the new design, the AR camera is only set to ``active'' only when the AR
panel is being shown. This prevents us from incurring unnecessary computational
cost when the AR camera was not in use.

Another group of challenges were created by the content of the main screen,
which showed the images and audio transcript associated with each region. To
display the images, we wanted the user to be able to click through them like
an image gallery.

We created the gallery out of a horizontal layout group, populated it with
images dynamically in each region, and then had the buttons ``lerp'' the gallery
to different positions in order to switch between each image in a smooth and
pleasing way. As for the audio transcript, we knew we needed a scrollable text
field that could accommodate varying amounts of text, since the audio files
varied so greatly in length. We created a vertical layout group to create a text
field that expanded vertically within predefined bounds to fit an arbitrary
amount of text.

The last major UI implementation obstacle we had to overcome was a way to keep
audio playing with the phone in your pocket. Atticus insisted that this sort of
feature was required so that users could listen to the audio tour without using %maybe include an image of pocket mode here
any of the other functionality the app provides, like the image gallery in each
region or the AR features. Despite how common it is for apps to play audio in
the background or while the phone is locked, Unity does not provide a way to
accomplish this. Our workaround was to create a ``pocket mode'' that prevents UI
elements from accidentally being triggered. The user exits pocket mode with a
deliberate swipe to the right. If the slider is released partway through the
action, the slider springs back to the beginning. We were ultimately able to
replicate Atticus's Adobe XD file almost perfectly.

\clearpage

\section{Testing}
\label{sec:testing}

Both pieces of our project---Editour and ARuko---are designed to be used by
people who are not programmers. Editour will be used by Atticus in the future to
design more tours, and ARuko will be used by the general public. Because of
this, we needed to do extensive testing to ensure not only that the products
work as intended without bugs, but also that they would be pleasant and easy to
use for their intended users.

% TODO remove this
\nocite{harvey2002} % don't remove this

\subsection{Testing Editour}
\label{sec:testingEditour}

Because Editour is simply an internal tool, our user testing was less rigid.
Even so, the frontend and backend we built for Editour ended up being reliable.
The few reliability issues stemmed from the hosting provider, which was out of
our control.

\subsubsection{Unit Testing the Editour Backend}
\label{sec:unitTestingTheEditourBackend}

To test the API and backend of the Editour web app we used the Mocha testing
framework. Mocha is the most depended-upon package in the Node Package
Manager~\autocite{tidelift2019}, and is widely used by professional software
development teams. It has a myriad of useful features that allow developers to
quickly write unit tests for complex Node.js projects~\autocite{mochajs2019}.
Most importantly for us, it has good support for testing asynchronous JavaScript
functions, since most of the backend is written asynchronously. Along with Mocha
we also used SuperTest, a library for writing high-level end-to-end HTTP
tests~\autocite{supertest2019}.

For each of the six API endpoints that the backend responds to, we wrote an
extensive series of end-to-end tests to ensure that each step along the pipeline
from receiving a request to sending a response worked properly. We also wrote
many fuzz tests to make sure the backend would respond correctly to invalid
requests, for example, by responding with an HTTP 400 error
message~\autocite{rfc7231}.

These tests were very useful in tracking down bugs and unexpected behavior in
the backend, and also allowed us to make sure no functionality changed when
adding new features or making changes to underlying logic later on.

\subsubsection{Editour Frontend Testing}
\label{sec:editourFrontendTesting}

We suspect that Editour might have a life outside of this particular use case
depending on how it is developed later on. This is why Editour is perhaps more
polished than your typical tool purely for internal use.

For the duration of the project however, Editour only had to be used our team
and Atticus. Therefore, the testing for the frontend of Editour was less
systematic than other parts of the software stack. This decision also helped us
focus our efforts on testing the reliability and user experience of ARuko.
Still, we were able to add many features and get the Editour experience to a
polished state through our own testing, using GitHub's tools to keep us
organized.

In order to test for bugs, all three of us periodically interacted with
successive builds of the frontend. When bugs were found, we would log these as
``Issues'' on the GitHub repository.

Since we had to use Editour frequently to create test tours for ARuko, we were
able to identify features that would make our own work more efficient. This
worked as a kind of informal user testing. We kept track of feature requests
using the same system as bug reports.

Issues and feature requests alike got resolved in future pull requests. We
resolved over thirty issues and feature requests using this workflow. The GitHub
issues tab (which can be see in Figure~\ref{fig:issuesPageExample}) was very
useful for ensuring we made use of our user testing and bug testing.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{issues-page-example.png}
	\caption{The GitHub issues tab for Editour}
	\label{fig:issuesPageExample}
\end{figure}

\subsection{Unit Testing ARuko}
\label{sec:unitTestingARuko}

The UI and AR features of the app are very important to the user experience. At
the same time, ARuko is not as purely graphical as it may seem. There is a
significant amount of ``business logic'' that interprets the tour from the
Editour file. On top of this, the geometric tests to check whether the user is
inside a particular arbitrarily-defined geographic region are very important to
get right.  Using Unity's built-in NUnit unit testing framework, we wrote tests
to verify whether the tour data was being parsed correctly, and whether the tour
would be run correctly.

Regions are a set of three or more points that define a closed polygon. Using
the user's GPS location, the app needs to periodically run a check to see if the
user is located within one of those polygons.

% TODO move the stuff about the algorithm into the implementation section
One decent algorithm for accomplishing this is by drawing a ray from a given
point to infinity. Barring edge cases (which will be discussed shortly) the
point is inside the polygon if the ray passes through an even number of line
segments.  The point is outside of the polygon if the ray intersects an odd
number of times. Figure~\ref{fig:intersectionsDiagram} does not serve as a
proof, but should at least make it clear why this works. This algorithm's time
complexity is $O(n)$, where $n$ is the sum total of the number of vertices of
all of the polygons being tested~\autocite{geeksforgeekspolygon}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{intersections-diagram.png}
	\caption{A visualization of the algorithm}
	\label{fig:intersectionsDiagram}
\end{figure}

For this, we implemented a few mathematical functions for individual parts of
the algorithm, such as one that tests to see if two lines intersect. We wrote
unit tests to verify each part of the algorithm. This gave us confidence that
the GPS component of the app would place users into the correct regions.

There is one edge case where this algorithm has difficulties. If the ray that is
drawn for testing intersections passes directly through a vertex, the algorithm
will count two intersections instead of just one. Even though this is highly
unlikely considering our use case, we decided that our algorithm should be
robust enough to handle this on a matter of principle. We solved this by running
a check on all vertices in the polygon to see if any were exactly colinear with
the ray. If this is the case, one intersection count is subtracted from the
total. Unit testing helped us catch and deal with this edge case.

\subsection{Testing Taking a Tour with ARuko}
\label{sec:testingARuko}

Because it was not feasible to travel to Kinkaku-ji every time we needed to test
the GPS functionality of the app, we used Editour to design an example tour
around the Ritsumeikan Biwako-Kusatsu campus where we worked. This allowed us to
catch various bugs before we created a finalized prototype for field-testing at
Kinkaku-ji. The tour represented in Figure~\ref{fig:ritsuTour} is what we used
for testing the app. The ``C-Shaped Region'' helped us test concave regions.
This uniquely shaped region also prompted us to think about what UI behavior
made the most sense if a user were to walk into a region, walk out and then in
again. We decided that it was awkward if the audio restarted upon re-entering a
region, since the audio would still be playing once a user left the region. As
we began to implement more complex UI that changed based on GPS position in
various ways, directly taking the tour by walking around was an invaluable way
to test. Section~\ref{sec:adobeXdDesign} details how the UI grew in complexity
in the late stages of the project. Having a way to regularly run field tests was
integral to polishing the app in time before we revisited Kinkaku-ji.  Because
the Editour was flexible enough to easily swap out media and add regions to the
tour, the Editour proved itself to be as helpful for testing as it was for
crafting the final tour that could be taken at Kinkaku-ji.

To maximize testing efforts and negate having to physically walk around we also
simulated the GPS coordinates in software. This allowed us to simulate walking
in a single direction without actually having to build to a mobile device or
move.

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{ritsu-tour.png}
	\caption{The Ritsumeikan example tour used for testing}
	\label{fig:ritsuTour}
\end{figure}

\subsubsection{Initial Field Testing}
\label{sec:initialFieldTesting}

Our first day of on-site testing with the app took place on September
6\textsuperscript{th}. Using Editour, we rebuilt the izi.TRAVEL audio tour that
Atticus had created previously. The first version of the reimplemented tour had
regions that were relatively small; some regions were only a few meters across.
This version of the tour can be seen in Figure~\ref{fig:kinkakujiTour}. On this
day of testing, we still had the limited UI described in
Section~\ref{sec:earlyUserInterface}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kinkakuji-tour.png}
	\caption[The original version of the Kinkaku-ji tour]{The original version
		of the Kinkaku-ji tour with small regions}
	\label{fig:kinkakujiTour}
\end{figure}

Due to concerns about GPS accuracy, we adapted the tour-file \texttt{kinkakuji}
to create a second, ``chunkier'' tour.  In the tour-file
\texttt{kinkakuji-chunkier}, we expanded all of the regions to accommodate for
potential GPS inaccuracies that could place the user outside the range of the
region.  Figure~\ref{fig:kinkakujiChunkierTour} depicts the tour that we used
for field testing at Kinkaku-ji.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kinkakuji-chunkier-tour.png}
	\caption[The ``chunkier'' version of the Kinkaku-ji tour]{The ``chunkier''
		version of the Kinkaku-ji tour with larger regions}
	\label{fig:kinkakujiChunkierTour}
\end{figure}

We tested using two phones at the same time: an iPhone 8 and a Samsung Galaxy
S7. The iPhone 8 is newer, more powerful and therefore more suited for use with
AR. We primarily used this phone to test the AR features of the app. Even
though the Galaxy S7 had difficulty with basic AR features such as
ground-plane detection, it was still capable of triggering the audio track
based on GPS position.

As discussed in Section~\ref{sec:testingARuko}, our previous testing involved
creating a ``mock tour'' on the Ritsumeikan campus. While this let us know
generally which features of the app were working, it was very difficult to know
how ahead of time how well our app would work on-site.

We were surprised by how intuitively each region triggered on the first try.
With a few exceptions, the audio started when the subject of the audio track
was in view. During our first walkthrough of Kinkaku-ji, we triggered every
region and audio track but one. The region that we missed on the first pass,
``Final Viewpoint,'' requires the user to take a right at a fork in the road
instead of continuing along the main path. This is not necessarily a problem
with the design of the regions; instruction provided by the audio tour could
remedy this, directing the user toward the viewpoint that is off of the main
path.

Initially, we loaded the app onto the Galaxy S7 as a backup, since the AR
features were not fully functional on a phone that old. At the same time, it was
informative to take both phones through the tour. Initially, we made the design
decision to update the GPS location every 60 frames. On a phone running the app
at full speed, this should happen once every second. Because the Galaxy S7 was
having trouble running the app at full speed, the GPS updated far less
frequently. This was an oversight that would have been difficult to catch if we
had not tested on hardware below the recommended specifications.

Other bugs resulted from oversights that were difficult to catch with our
on-campus testing. For example, we noticed that we forgot to reset the audio
slider when entering a new region.  Because of this, when we entered a new
region when the audio slider was past the endpoint of the new audio track, the
audio would not play. For example, if the previous audio track were a minute
long, and if the second track were only thirty seconds long, the app would try
to play a thirty second audio track from the one minute mark. Since we were
running the app on phones, we could not easily see when errors were thrown.
Similarly, if the user were to enter a new region while thirty seconds into the
previous audio track, the new audio track would start at the thirty second
mark. During our on-campus testing, we were using songs rather than narration.
Since the songs were relatively long and not always obvious when they started
partway through, we did not catch this until we got out to Kinkaku-ji. Luckily,
this was trivial to fix.

\subsubsection{Final Field Testing}
\label{sec:finalFieldTesting}

On September 26\textsuperscript{th} we returned to Kinkaku-ji with a nearly
complete version of ARuko in order to conduct some final testing. We met up with
Atticus, the IQP group, and our friend Hikaru and installed the beta version of
ARuko on each of their phones. With little instruction about how the app works
we had each of them try going through Kinkaku-ji while listening to the tour and
using the app's features. Each member of our team also installed the beta
version so we could test it. % TODO add photos of us testing it

This was our first time trying the AR image recognition on the actual signs at
Kinkaku-ji, rather than photographs of the signs, and it worked surprisingly
well. Vuforia was able to recognize almost all of the signs, even in different
lighting than when the reference images were taken. For image targets that were
small, far away, or obscured by shadows the image recognition was slower. We
discovered that the targets that worked best were large, had a lot of
high-contrast text, and were uniformly lit.

While testing we discovered some issues with the design of the tour. One
region was completely misplaced, so the audio played at the wrong time while
walking through the area. Several other regions were too big, so the audio
started playing long before the user was actually near the feature it was
discussing. Using the Editour web app, we were able to quickly correct these
issues with the tour design once we returned to Atticus's office.

Additionally, one of the image targets had the wrong text in its information
pop-up. These were all easy bugs to fix, but if we might not have noticed
them if we had not tested the app on-site.

We also noticed that we were only able to augment a maximum of two image targets
at a time. In almost all cases, only two translatable signs were in view at
once. The only scenario in which there were more than two image targets in view
at a given time was with the four framed photos of the interior, which were
arranged in a two by two grid. When standing at a distance from the grid, only
two images would be augmented with the blue information button. By default,
Vuforia limits the amount of image targets that the engine should accept
simultaneously. In the Vuforia engine settings, we changed this limit to
accommodate for four simultaneous images.

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{four-i-buttons.png}
	\caption{The result of increasing the size of the info buttons and increasing
    the maximum image target count to four}
	\label{fig:fourIButtons}
\end{figure}

Another major bug we discovered was that the GPS stopped updating when the main
screen is not open. This means that in AR camera mode the app would not trigger
new audio when you walked into a new region. This was caused by the GPS script
being attached to a UI element on the main screen, which gets deactivated when
switching to AR camera mode. This was easily resolved simply by moving this
script.

After having Atticus go through the tour on his phone he also discovered several
bugs and tweaks he wanted to make to the UI. The clickable information buttons
on image targets were made larger so they are easier to see and tap. The UI
buttons in the bottom menu bar were also made larger and easier to press. The
text ``MAIN'' was added under the central button that opens the screen with the
images and transcript, so it was more clear what that button does. The header
text at the top was too big, and sometimes overflowed onto other UI elements, so
its size was reduced and split into two lines when the text was too long.

Testing with Atticus revealed another user interface issue we had not
considered.  Many modern smartphones, such as Atticus's Samsung Galaxy S10+,
have holes or notches cut out of the screen for cameras, speakers, and other
hardware (see Figures \ref{fig:galaxyS10Plus} and \ref{fig:iPhoneX}). We had
been designing our UI for rectangular phone screens, so the Galaxy S10+'s ``hole
punch'' covered some UI elements in the top right corner. To account for this
issue we used Unity's built-in function to return a safe screen space to render
the rectangular canvas in.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{galaxy-s10+.jpg}
	\caption[A Samsung Galaxy S10+ phone with distinctive ``hole punch''
	cameras]{A Samsung Galaxy S10+ phone with distinctive ``hole punch''
		cameras~\autocite{yoo2019}}
	\label{fig:galaxyS10Plus}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{iphone-x.jpg}
	\caption[An iPhone X with a large notch]{An iPhone X with a large
		notch~\autocite{yoo2017}}
	\label{fig:iPhoneX}
\end{figure}

After going through Kinkaku-ji with the app twice we had each test participant
fill out a simple survey giving their thoughts on the app. The full results of
these surveys can be found in Appendix~\ref{sec:testingSurveyResults}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{kanban-board.png}
	\caption{A screenshot of the Kanban board we used for developing our
    biweekly blog}
	\label{fig:kanbanBoard}
\end{figure}

\clearpage

\section{Alternative AR App Concepts}
\label{sec:alternativeArAppConcepts}

Throughout the duration of the project, the goalposts shifted multiple times.
This section details some of the initial plans that we explored before Atticus
shifted our focus to a walking tour app with AR features.

\subsection{3D Spatialized Audio Tour}
\label{sec:3dSpatializedAudioTour}

% TODO is spatialized really a real word or do people just use it?
One idea that was proposed initially was an AR audio experience using 3D
spatialized audio. Instead of traditional AR where the user is constantly
looking through the camera, this experience would allow users to keep their
phones in their pockets while taking the tour. Sound sources would change in
relation to the real world as the user moved around.

We spent the second and third week of the project creating demos to explore the
possibility of a 3D spatialized audio tour. We found that this would be very
difficult to do without the use of the camera. All of the technology for mobile
AR we explored in Section~\ref{sec:platforms} depended heavily on the camera. If
we were to build a 3D spatialized audio tour that ran entirely without use of
the camera, we would not be able to take advantage of any of the useful features
of the AR libraries we explored. Furthermore, Unity would be relegated to the
simple role of exporting our app to multiple devices; we would not have any need
to take advantage of its 3D capabilities.

Thusly, we decided to see what was possible using the camera instead. We were
able to get sound sources to fade in and out based on the distance to an image
target. Naturally, if the image target were to go out of view of the camera,
Vuforia stopped trying to estimate the camera's distance to the last known
location of the image target. In order to make this work in the way Atticus
imagined, we would need a way to shift to our own method of estimating distance
to the sound source once the image target had been lost. While this might have
been possible, it was difficult to estimate the amount of effort this would
take, since we would essentially be implementing a feature seemingly not
supported by current AR libraries.

We explained the technical hurdles of this concept to Atticus. Ultimately, our
focus shifted away from this idea for other logistical reasons. Initially,
Atticus introduced the possibility of working closely with artists Kishi Bashi
or Yannick Paget to produce the soundscape for the 3D spatialized audio tour.
Yannick Paget is a French musician that became established in Japan. He is both
conductor and composer, originally trained as a professional pianist and
percussionist. He has written music for theater, film and orchestras around the
world. Many of his original compositions were inspired by his experience living
in Japan, such as Tears of Sakura
(\begin{CJK}{UTF8}{min}サクラの涙\end{CJK})~\autocite{yannickpaget2016}. Kishi
Bashi was born in Seattle and grew up in Virginia. Kishi Bashi has appeared at
many major music festivals, toured the US extensively, and has even released his
own line of coffee. His latest album, Omoiyari, was released on May
31\textsuperscript{st}, 2019~\autocite{kishibashi2019}.

\subsection{AR Art Gallery}
\label{sec:arArtGallery}

Atticus Sims proposed the idea of working with Michael Whittle, a British artist
received his PhD at the Kyoto City University of Arts in 2014. His art has
appeared in dozens of solo and group exhibitions across the world,
including Kyoto, Beijing, London and New York~\autocite{michaelwhittle2019}.
Kyoto~VR has worked with Michael Whittle in the past to create a 3D scan of a
solo exhibition, ``Portraits of Thought.'' This exhibition displayed diagrams
inspired by real science and mathematics. Atticus provided us with some
diagrams to test in the Vuforia developer portal, since Michael Whittle wanted
to know how well certain pieces of art would work as AR image targets. All of
images that were provided to us were ranked as ``five stars'' by Vuforia,
indicating that those images would serve well as AR image targets.

\begin{figure}[h]
	\centering
	\includegraphics[width=.7\textwidth]{exhibition-scan.png}
    \caption{A screenshot of the 3D scan of `Portraits of Thought,' accessible
    from the web}
	\label{fig:exhibitionScan}
\end{figure}

\subsection{Alterniative Uses for AR in ARuko}
\label{sec:alternativeARukoAR}

Even once the idea for ARuko was settled on audio tour functionality with 
additional AR components, what exactly those AR components would do was still
undecided for a while. Atticus knew he wanted to use AR on the tour, but wasn't
sure what to do with it. At first, we made recommendations to him based on the
capabilities of Vuforia that we found in our research. We knew it was good at
image recognition and ground plane recognition, so we suggested that maybe 
displaying some sorts of 3D models in certain locations using either of these
features might be an interesting way to expand on the tour. With these 
recommendations in mind, Atticus expressed interest in the capability of using
building facades as image targets. The intention then would be to tie content
related to a specific location within Kinkaku-ji to that physical location, and
use images of the location as targets. As previously explained, this ultimately
did not work out due to the limitations of image recognition with Vuforia. 

After being initially deterred from image recognition, we thought that ground
and mid-air plane recognition might the best way to implement AR in the app. 
It didn't have to rely on images that meet Vuforia's criteria, and since it 
could be triggered anywhere, it seemed like a good fit for an app focusing on
walking around a site. Atticus explained that he primarily wanted to use AR to
display art related to the site. So we focused our efforts with ground and 
mid-air plane AR on ways to display 2D images. What we ended up creating first
was a feature that allowed you to view certain images in AR, using ground plane
detection, based on what region you were in. So when you entered a region of 
the tour, the image menu would be dynamically populated with buttons that, when
tapped, would create a Plane primitive in Unity with the given image displayed
on it and place it on the ground plane. 

We encountered a few notable problems with this though. One problem was that 
the Plane we made was always the same size and dimensions, and would cause 
images to stretch to fit the plane. Further testing to change the size of the
plane with each picture proved difficult. Additionally, using a Plane primitive
had problematic effects on the way the object was lit in the Unity scene, as 
well as how it appeared when not viewed head-on. If viewed from an angle, the 
picture could become very dark and difficult to see. And if viewed from behind,
it would actually be completely invisible. Because of these issues, we decided
to place the images on a 3D Cube primitive instead. But after showing this
feature to Atticus during early testing at Kinkaku-ji, he decided that he 
wanted to move away from displaying images in AR. Looking back, the feature
may have worked better with mid-air plane recognition so that the user could
place the image directly in front of them. 

In addition to the previously mentioned use cases for AR in ARuko, Atticus also
expressed interest in using AR to trigger video content. He was particularly
interested in 3D videos triggerable through AR. As the scope of the project 
grew, and time ran short, the task of including 3D videos triggered by AR 
seemed more and more complicated, causing that idea to be phased out of the
final project. Additionally, we never received any assets from Atticus to use
for this feature, thus preventing us from spending time developing it. 

\subsection{Monetization for ARuko}
\label{sec:monetizationForAruko}

Partway through our project, Atticus Sims let us know that he was looking to
monetize the app we were developing for him. He also informed us that the IQP
team would be looking for funding options in order to ``bootstrap'' the project.
One monetization method he proposed was potentially marketing the app to hotels,
allowing hotels to sell tours for download through in-app purchases.

In order to get an app approved for the App Store or the Google Play store,
there is sometimes a lengthy approval process. This might have been possible if
our project had been more prescriptive from the start.

We did perform some preliminary research about the monetization models Atticus
proposed and brought forth some potential roadblocks regarding each one. We let
Atticus know that we had logistical concerns about circumventing Apple's or
Google's in-app purchases by locking content behind a different payment process.
Additionally, we informed Atticus that he would have to purchase a commercial
Vuforia license in order to deploy the app onto any
storefront~\autocite{vuforialicense2018}, which Atticus was not willing to do at
the time. The free version of Vuforia comes branded with a watermark in the
bottom left corner.\footnote{The final design of our app incidentally covers up
the watermark with our own UI elements. This does not change the fact that we
would not be able to deploy the app without a proper commercial license for
Vuforia.}

Our concerns about focusing on monetization for Kyoto~VR were twofold. Firstly,
we were worried that this kind of work strayed too far away from the academic
intent of the project---the intent being to explore various ways in which AR and
geolocation could be used in tandem to create an improved experience for
tourists. For our purposes, the app was meant to demonstrate our discoveries in
the form of a polished prototype. While we feel as though we achieved this goal,
we made certain compromises in the way of experimentation in order to leave
Kyoto~VR with a potentially marketable product. We made it clear to Atticus that
deploying our app on the Google Play Store or App Store would not be within the
scope of our academic project as per our advisers' advice.

\clearpage

\section{Conclusion and Future Work}
\label{sec:conclusionAndFutureWork}

Our team is satisfied with the final result of this project, as we feel we have
built a working platform that Kyoto VR will easily be able to build off of in
the future. This section contains our final thoughts on the project and future
work that could be done on it.

\subsection{What Went Wrong}
\label{sec:whatWentWrong}

Although the finished product works as intended, we encountered several issues
along the way. The changing scope and goal of the project meant we spent
several weeks researching and working on features that did not make it into the
final product (see Section~\ref{sec:alternativeArAppConcepts}), such as
displaying images in 3D AR space and playing spatialized audio. If we had
focused only on the features that made it into the final app from the start we
could have had more time to polish and refine those features.

On a similar note, the requirements and scope of the project changed greatly
from week to week, and sometimes even within one day. We spent a significant
amount of time exploring concepts and features that were completely unrelated to
the final product. We also did not receive official specifications, such as the
design of the GUI Atticus wanted for the app, until very late in the development
process.

Another challenge we faced was the difficulty in actually testing the app. The
Ritsumeikan University Biwako-Kusatsu campus we stayed at is about ninety
minutes away from Kinkaku-ji by expensive public transport, which meant that we
were only able to go to the site to test ARuko twice. We could test some basic
features from Ritsumeikan, but the image recognition targets existed only at
Kinkaku-ji. More extensive user testing at the actual site could have helped us
catch bugs earlier and lead to a more polished final product.

We had some initial trouble getting started with Unity. Different versions of
Unity do not play well with each other. As a result, we often ran into bugs on
older hardware, such as the laptops we had to work with. Furthermore, a stable
version of the Unity editor is not available on GNU/Linux~\autocite{best2019},
so one of our team members had to install macOS in order to use it. We were able
to resolve these issues in the first weeks of the project, but it consumed some
of our valuable time in July.

\subsubsection{What Went Well}
\label{sec:whatWentWell}

Despite these issues, in the end we were able to create a useful working
prototype. The Editour is fully featured with everything needed to build an
audio tour anywhere in the world. ARuko allows users to experience useful
AR-enhanced tours in a pleasant and intuitive way. Together they provide a
complete platform for building augmented reality tour guide apps.

Our team's broad range of skills made it so that each member had specific tasks
that they were good at. This made the division of labor easier, and throughout
the project we never had issues splitting up work.

Editour was not initially a requirement that Atticus had in mind, but it turned
out to be a very useful tool in designing tours and allowing the app to be
extensible. Two members of our team, Cole Granof and Joseph Petitti, had
experience building web applications with HTML, CSS, and JavaScript, and the
Editour allowed them to use these skills to help the project.

The research time we put in at the beginning of the project also proved to be
very useful. Our decision to use Unity Vuforia based on this research made
implementing many of the complex AR features easier later on. If we had chosen a
less-ideal platform, we might have had to switch to a different one after
wasting time on that poor decision.

\subsubsection{Future Work}
\label{sec:futureWork}

Although selling ARuko on app stores was not within the scope of our academic
project, but we were able polish the app enough such that it would be feasible
for Atticus to eventually sell the app. The UI could use more testing with real
users to make sure it is intuitive and easy to use. We also intended to add a
feature where the app could automatically download tour files from the Editour
server and import them without having to rebuild. We were not able to complete
this feature due to time constraints, but it could easily be added because all
of ARuko's business logic is abstracted and generalized to support multiple
tours. Editour also already supports requesting and downloading the most recent
versions of tours.

Editour is essentially feature complete. The most recent version, v1.3.0, can be
found in the GitHub repository
(\url{https://github.com/bandaloo/editour/releases/tag/v1.3.0}), and it contains
all the features necessary to create and edit tours. In the future Kyoto~VR
should move it to a more powerful hosting provider (see
Section~\ref{sec:deployingTheEditourBackend}), but the current solution works
for now.

Given more time and resources, the app should be augmented to include a tour
browser that automatically downloads an unpacks tours from the Editour server.
More historic locations around Kyoto and Japan in general should be considered
for other tours. With the work our team has done, Kyoto~VR now has a complete
platform for creating and delivering unique tours, which can easily be expanded
upon in the future.

\clearpage

% references should be single spaced
\begin{singlespace}
	\printbibliography
	\addcontentsline{toc}{section}{References}
\end{singlespace}

\clearpage

\appendices

% TODO should this really go in the appendix?
\section{Version Control}
\label{sec:versionControl}

Over the course of the project, we had to develop multiple applications in
different languages. In total, we created five different repositories: the blog,
the \LaTeX~source code for our paper, the Arduino code for the heart project
(see Appendix~\ref{sec:ritsumeikanHeartProject}) and of course one for Editour
and another for ARuko. These five repositories span across multiple different
markup and programming languages, including C\#, C++, JavaScript, HTML, CSS (and
the \LaTeX~used to typeset this paper). This gave us a welcome amount of
practice using Git for version control, as well as GitHub's other features
oriented around coordinating a team.

All three members of the team were contributing code and updating assets such as
graphics and sound. Code complexity grew quickly as more features were added
across the entire software stack. GitHub and Git enabled us to view the history
of the code and revert back to working versions when absolutely necessary. We
researched the proper methodology of using version control in a small team,
applying this methodology as best we could. This enabled us to work together
efficiently, track down bugs, undo mistakes, and review each other's code for
quality when time allowed.

\subsection{GitHub Tools}
\label{sec:gitHubTools}

Some members of our team were very excited by the workflows GitHub's tools
enabled. In this section, we will discuss some of the tools, some of which
greatly aided in our organization and productivity.

Initially, we experimented with using a four-column Kanban board to keep track
of our progress. Figure~\ref{fig:kanbanBoard} shows a screenshot of this page on
GitHub.

The Kanban technique stems from Toyota in the 1940s. Kanban means ``visual
signal'' or ``card'' in Japanese, which describes the core component of the
technique~\autocite{terry2019}. At Toyota, line-workers would move colored cards
to notify other teams that parts needed for assembly work were in short supply.
Now, Kanban boards are integral to Agile software development. Some teams still
use physical Kanban boards, often using sticky notes and whiteboards.

The advantage of using a Kanban board is that each feature or bug that
is being worked on can be assigned directly to a collaborator on the project. It
is then that person's responsibility to work on those tasks and update the
status of those tasks on the board. We abandoned this when moving onto the main
project because we came to realize that this structure was not necessary---there
were only three members on the team, and we were almost always in the same room
when working on the project. For Editour, we tracked features and issues using
the ``Issues'' page instead, which is discussed in
Section~\ref{sec:editourFrontendTesting}.

Creating ``pull requests'' through the GitHub web interface has a few advantages
over directly merging code using Git. Firstly, the pull request feature
integrates with ``Issues'' page very well. By typing `resolves' along with the
number associated with an issue into the description of a pull request, GitHub
will automatically close the issue when the pull request is approved. This is
not only remarkably convenient, but also self-documenting since the descriptions
of pull requests would contain the issues they were resolving. Secondly,
creating a pull request allows code to be reviewed by other members of the team
very easily. Reviewers can view the entire diff and add comments line-by-line.
Other collaborators can easily respond to those comments. Reviewers can also
request certain features be changed before the branch is merged, which allowed
us to enforce that issues with the code were actually addressed.
% TODO explain what the heck a diff is

% TODO give this paragraph some more context so it doens't seem out of place
The way we incorporated branches into our workflow helped us collaborate in a
way that kept us organized. In general, if one of us was working on a specific
feature, we would create a branch with a name descriptive of that feature. This
methodology minimized difficult-to-resolve merge conflicts. We modeled this
workflow off of the ``Git Feature Branch Workflow'' Atlassian
tutorial~\autocite{atlassian}.
% TODO explain merge conflicts

In order to clearly indicate new versions of the software, we use ``Semantic
Versioning'' which is a scheme for version numbers created by Tom
Preston-Werner, cofounder of GitHub~\autocite{prestonwerner}. In this scheme,
the full version number consists of three integers separated by periods---in
other words, in the form \texttt{major.minor.patch}. The major version number is
incremented when an incompatible change is made. In our case, an incompatible
change for Editour would entail changing the format of the metadata file used to
construct a tour such that the frontend could not interpret older tour files.
The exception to this rule is major version 0, where the product is not
considered stable and incompatible changes can be made at any time. When Editour
was in this phase of development, we changed the format of the metadata; this
would have broken previous versions but this was allowed. The minor version
number is incremented when functionality is added but remains backwards
compatible with previous versions. An example of this is the ``transcript''
field we added to the metadata. The frontend was still able to load tours made
before this additional information was added to the metadata JSON. We added many
other features throughout the development process without breaking existing
functionality. The patch version number is for simple bug-fixes that also do not
break compatibility. This number gets incremented the most frequently in
practice.

Once we deployed Editour to a server so that Atticus could begin to use it, we
changed the version to 1.0.0 to indicate the first release version. At this
point, we were committed to not making any backwards-incompatible change. Since
Atticus would be interacting with Editour after the release of 1.0.0, it was
important that the master branch remain as stable as possible. Because of this,
we created a ``dev'' branch where we could test experimental features to improve
the experience and functionality of Editour. Maintaining a stable branch along
with a separate experimental branch reduced the risks involved with continuing
to iterate on the software's features post-release.

In a similar vein, with ARuko we used branches to ensure we had a stable working
version. The night before field testing at Kinkaku-ji, we would cease work on
the most recent branch, and build the app to our phones. This way, we would
always have an exact snapshot of the code we used for that day of testing.

\clearpage

\section{Editour API Documentation}
\label{sec:editourAPIDocumentation}

Here is a list of API functions the Editour backend handles. The response is
usually a JSON like this with fields called \texttt{status} and \texttt{message}
The status is an HTTP status code like 200 (OK) or 404
(Not~Found)~\autocite{rfc7231}, and the message is a string or stringified JSON
that contains the data of the response.

\noindent\textbf{GET \texttt{/edit/:name}}

\begin{indented}{1cm}
	Returns the metadata file of the most recent tour with the given name. If
	successful, it returns a status of 200 and the stringified metadata as the
	message. If no tour with the given name is found it returns 404, and if the
	server encounters an internal error while processing the request it returns
	500.
\end{indented}

\noindent\textbf{GET \texttt{/tour/:name}}

\begin{indented}{1cm}
	This is the only API endpoint for which the response is not a JSON. Instead,
	the response is the zip file of the most recent version of the tour with the
	given name. Returns 404 if a tour with the given name isn't found or a 500
	if there is an internal server error while processing the request.
\end{indented}

\noindent\textbf{GET \texttt{/tours}}

\begin{indented}{1cm}
	Gets a list of unique tours on the server, returning a list of their names
	as the message of its response. Returns 200 if successful or 500 if there is
	an internal server error while processing the request.
\end{indented}

\noindent\textbf{POST \texttt{/edit}}

\begin{indented}{1cm}
	Used for editing an existing tour. This request should contain the fields
	\texttt{tourName}, \texttt{oldName}, and \texttt{metadata}, along with any
	number of files.  The server will use the new metadata as well as the old
	and new files to create a new version of an existing tour. Returns 201 if
	the tour was edited successfully, 400 if the request was invalid, 404 if the
	requested tour could not be found, or 500 if a server error occurred while
	processing the request.
\end{indented}

\noindent\textbf{POST \texttt{/upload}}

\begin{indented}{1cm}
	Used for uploading a new tour. This request should include a
	\texttt{tourName} field and a \texttt{metadata} field, along with any number
	of files. It verifies the uploaded tour and creates a zip for it on the
	server. Sends a 201 if the tour was created successfully, a 400 if the
	request is invalid, or a 500 if a server error is encountered.
\end{indented}

\noindent\textbf{DELETE \texttt{/tour/:name}}

\begin{indented}{1cm}
	Deletes all versions of the tour with the given name from the server.
	Returns 200 if successful, along with a message saying how many versions
	were deleted, 404 if no tour with that name could be found, or 500 if a
	server error was encountered.
\end{indented}

\clearpage

\section{Ritsumeikan Heart Project}
\label{sec:ritsumeikanHeartProject}

Throughout our stay at Ritsumeikan, we also had the opportunity to work closely
with other Ritsumeikan students to create build a project that involved the
integration of hardware and software. With the help of students and professors
alike, we were able to produce an educational physical model of a heart
controlled by an Arduino microcomputer. Noma Sensei gave us tremendous
assistance with hardware. He taught us how to properly use the tools around the
lab, and purchased mechanical and electrical components we would need to
complete our task. Amano Sensei was always available to answer questions related
to biology, and helped us understand the mathematical model we would need to
implement to drive the physical model with software. We worked on this project
every Wednesday, completing the final model during the last week of our stay.
% TODO get the full name of Amano Sensei

\subsection{Project Goal}
\label{sec:projectGoal}

The goals and requirements of this project were set out by Haruo Noma, a
professor at Ritsumeikan University. The goal of the project was to create an
interactive model of a heart to teach heart function to non-specialists, such as
students in middle school. This meant that the model had to be highly visual and
simple to understand. Each visual component would have to be designed in an
intuitive fashion so that students could begin interacting with the model
without advanced knowledge of biology.

\subsection{The Heart Simulation}
\label{sec:theHeartSimulation}

We were presented with a set of equations that defined the mathematical model of
the heart we would be using to drive our physical model. Along with the
differential equations, we were also given an analogous circuit model that could
also be described by the same equations. The circuit and the equations can be
seen in Figure~\ref{fig:referenceModel}. We were also provided with source code
for an implementation of this model in C. We would still need to modify this
code to calculate the output variables we would need to properly operate the
physical model, and integrate the model into the main loop of the Arduino source
code with the appropriate timing.

\begin{figure}[h] \centering
    \includegraphics[width=0.4\textwidth]{reference-model.png}
    \caption{The reference model we used to implement the simulation}
    \label{fig:referenceModel}
\end{figure}

\subsection{Supplies}
\label{sec:supplies}

With Noma Sensei as our guide, we visited ``Den-Den Town'' or Nipponbashi in
Osaka in order to gather supplies we would need in order to construct the heart.
One of the most important purchases of this trip was a two-meter long strip of
Adafruit NeoPixel light emitting diodes (LEDs). The design of this reel of LEDs
enabled us to cut the strip at specific points to create shorter strips. Each
LED on the strip individually addressable, allowing us to set the color of any
LED along the strip. This perfectly suited our needs for representing blood
flow.

% TODO give a citation about what an Arduino is
% TODO do we need citations about every microcomputer mentioned by name?
Noma Sensei had various microcomputers for us to experiment with in his lab,
including an obniz, an M5Stack, a Raspberry Pi and an Arduino. The Arduino was
the only computer that had a ``motor shield'' component which we believed would
be necessary for controlling the pumps.\footnote{In the end, we did not need
this component, since we abandoned the idea of taking apart a pump to drive the
DC motor directly.} Because of this, we chose the Arduino to be the core of the
physical model. This microcomputer ran the simulation and output data to the
various hardware components, such as the digital display, LED strips and switch
to control the air valves.

\subsection{Interactive Heart Model Design}
\label{sec:interactiveModelDesign}

The design for the interactive model is pictured in
Figure~\ref{fig:interactiveModel}. The most prominent feature of the model is
the plastic bottle containing the balloon. The volume of this balloon represents
the volume of the left ventricle. Instead of inflating the balloon with the air
compressor directly (pictured on the left of Figure~\ref{fig:interactiveModel}),
we opted to pressurize a sealed two-liter soda bottle and suspend a balloon
inside. By pumping more air into the bottle, we could reduce the volume of the
balloon since the external pressure would be made greater than the internal
pressure; the balloon would shrink as a result. To accomplish redirecting the
flow of air, we designed a setup involving two valves.
% TODO do we need to cite something to explain the super simple physics of this?

\begin{figure}[h] \centering
    \includegraphics[width=\textwidth]{interactive-model.png}
    \caption{A diagram representing the physical model of the heart}
    \label{fig:interactiveModel}
\end{figure}

Like the balloon which represents the volume of the left ventricle, the LED
strips represent both pressure and blood flow in different parts of the body.
Pressure is represented by color. The lower the pressure, the more blue the
lights will appear. Red represents high pressure. The LEDs can display any color
in between blue and red, so the light strips are often a pinkish hue.

We were able to program custom animations by updating the color and intensity of
the LEDs in a rapid fashion. We represented blood flow by changing the phase of
a sinusoidal wave over time.\footnote{For clarity, we elongated the ``troughs''
of the sine waves, so the pattern of intensity was not a true sine wave.} Our
design included two strips of twenty LEDs. The LED strips are represented by the
red arrows on Figure~\ref{fig:interactiveModel}. The rate of the animation on
the right strip of LEDs represents blood flow out of the left ventricle (whose
value is $f_{1}$ in the model). The color corresponds to $P_{a}$ in the model,
which is the pressure in the aorta. The animation speed of the LED strip leading
back into the heart is controlled by $f_{v}$, the flow back into the ventricles.
The pressure corresponding to the color of this strip is $P_{v}$, or venous
pressure. In our model, this value is not an output variable. Instead, $P_{v}$
is held constant. Consequently, the color of this LED strip does not change over
time.

The physical model also contains a digital number display, which can be seen in
the top right corner on Figure~\ref{fig:interactiveModel}. The display shows the
number of the active scenario. Clicking on a scenario button in the GUI
(discussed in Section~\ref{sec:userInterfaceForHeartModel}) will change the
number being displayed.

\subsection{User Interface for Heart Model}
\label{sec:userInterfaceForHeartModel}

Our initial plan for the user interface was to control input and output
parameters with dials. The numerical values of the input parameters would have
been displayed on an array of digital number displays. Due to time constraints,
we opted to support five preprogrammed scenarios that could be toggled from a
GUI. Figure~\ref{fig:heartGui} represents the final version of the GUI we use to
interact with the physical model.

The red buttons along the top of the screen allow the user to toggle between
different scenarios. Each scenario also includes a helpful graphic below each
button.

\begin{figure}[h] \centering
    \includegraphics[width=\textwidth]{heart-gui.png}
    \caption{The GUI to enable five preprogrammed scenarios}
    \label{fig:heartGui}
\end{figure}

Not only can the GUI communicate with the Arduino; the Arduino can also relay
information about the state of the model back to the GUI. The length of blue bar
on the left of Figure~\ref{fig:heartGui} corresponds to $f_{v}$ in the model,
while the blue bar on the right corresponds to $f_{1}$. On the physical model,
these values correspond to the rate of animation of both LED strips. The size of
the circle between the blue bars represents $V_{LV}$, the volume of the left
ventricle. Due to limitations about the accuracy with which we could control the
pump, the volume of the balloon does perfectly reflect the true value of
$V_{LV}$ in the model. By contrast, the radius of the graphic on-screen can be
set to be exactly proportional to the value of $V_{LV}$.

% TODO write about the final result of the heart model and include pictures

\clearpage

\section{Testing Survey Results}
\label{sec:testingSurveyResults}

After testing a near-complete prototype of ARuko at Kinkaku-ji on September
26\textsuperscript{th}, we had each test subject fill out a simple survey with
their thoughts on the app. The full results are listed below.

\begin{table}[h]
\begin{singlespace}
\renewcommand{\arraystretch}{2.0} % add space between rows
\footnotesize % make font smaller
\begin{tabulary}{\textwidth}{ L | L | L | L | L | L }
	I enjoyed the audio tour & The image gallery added to the audio tour & I
	found the AR translation feature helpful & I found the AR map overlay
	helpful & The AR features were easy to use & Do you have any other thoughts
	on how to improve the experience? \\
	\hline

	Strongly agree & Agree & Strongly agree & Strongly agree & Agree & I thought
	the idea about a sound playing before the audio starts is a really good
	idea! \\

	Agree & Agree & Agree & Neutral & Neutral & Add a audio queue when a new
	track starts. Make the AR buttons bigger/easier to click. \\

	Agree & Agree & Strongly agree & Neutral & Agree & Increasing GPS accuracy
	of course. Adding a tracking ``you are here'' marker to the route. \\

	Strongly agree & Strongly agree & Strongly agree & Strongly agree & Strongly
	agree & Typo in the text on 4/11 towards the end \\

	Agree & Agree & Strongly agree & Disagree & Neutral & Zoom in ar mode \\
\end{tabulary}
\end{singlespace}
\caption{Results of final field test survey}
\label{tab:testingSurveyResults}
\end{table}

\section{NOxAR Assignment}
\label{sec:NOxAR}

On Monday, September 9th, Atticus approached us about an additional project he 
wanted us to tackle. He wanted us to adapt Kyoto VR's NOxAR app to use ground
plane detection. Currently, the NOxAR app is an AR app for Microsoft Hololens
that uses image targets with Vuforia. When viewing the correct image target,
the app projects a 3D model of a Noh Theater\footnote{A form of Japanese 
theater originating in the 14th century structured around song and slow 
dancing~\autocite{japanguide2018}.} performer dancing to background 
music~\autocite{noxar2018}. What Atticus wanted us to do was have the app use 
ground plane detection in order to display the Noh performer so that it could 
be displayed anywhere. Additionally, he wanted us to scale up the model so that
when displayed in AR, the performer would appear life-sized, as if she were 
really there dancing in the real world. 

For this project, Atticus sent us the original Unity project files for NOxAR.
We then had to figure out how to turn the original project into a ground plane
AR app. At first we tried importing the assets for the Noh performer into a new
Unity project. We did this for a couple reasons. The first reason was because
we didn't want all of the extra components of the original NOxAR project to
get in the way of what we were trying to do. We also did this because the %could probably put an example pic here
original project was made with a 2017 version of Unity, so we wanted to try to
use our more recent version so we wouldn't risk running into any depricated 
features or functionalities. We did eventually get this method to work, but the
model of the Noh performer was lacking a lot of the details that the original 
model had. This was attributed to poor importation of the original assets, so 
we decided to try making changes to the original NOxAR project instead. 

Per Atticus's advice, we deleted any and all unnecessary elements from the 
scene before proceeding. We also updated the Unity version of the project to
the 2019 version we were using, and updated the version of Vuforia to the most
recent version, neither of which created any problems. The process of getting
the model to appear on ground plane detection instead of image target 
recognition was relatively simple. We simply moved the model from an image 
target to a simulated ground plane, added a plane detector component to the
scene, and made sure the Vuforia settings were in place for ground plane
detection to work well. From there, we just had to test the app a few times so
we could properly scale the model to life size. So on Tuesday, September 17th, 
we completed the NOxAR ground plane adaptation to Atticus's saisfaction and
sent him the final APK file for the app. 

\end{document}

